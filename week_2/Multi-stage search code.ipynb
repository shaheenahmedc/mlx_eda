{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ac2da6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder, load_dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import string\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88a99c",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa745934",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"validation\")\n",
    "# dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"train\")\n",
    "df_train = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72d973",
   "metadata": {},
   "source": [
    "# Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf81818",
   "metadata": {},
   "source": [
    "Steps taken:\n",
    "- pip install sentencepiece\n",
    "- prepare data in required format (csv, new line per sentence)\n",
    "- run sentencepiece on corpus, to generate tokens\n",
    "- run sentencepiece embedding on sentences, to convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a CSV file for writing\n",
    "from tokenizer import prepare_sentencepiece_dataset\n",
    "\n",
    "# Write a csv file to disk, in the format expected by the SentencePieceTrainer\n",
    "prepare_sentencepiece_dataset(df_train, output_file = 'sentence_piece_input.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizer import train_sentencepiece\n",
    "# Define parameters for SP training\n",
    "input = 'sentence_piece_input.csv'\n",
    "model_prefix = 'mymodel'\n",
    "vocab_size = 4000\n",
    "character_coverage = 0.9995\n",
    "model_type = 'unigram'\n",
    "\n",
    "train_sentencepiece(input, model_prefix, vocab_size, character_coverage, model_type)\n",
    "\n",
    "# # Train the model\n",
    "# spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('mymodel.model')\n",
    "# Read in prepared SP input\n",
    "sentence_piece_input = pd.read_csv('sentence_piece_input.csv', header =None, names = ['sentence'])\n",
    "# Tokenize each sentence into tokens and token ids\n",
    "sentence_piece_input['tokenized'] = sentence_piece_input['sentence'].apply(lambda x: sp.EncodeAsPieces(str(x)))\n",
    "sentence_piece_input['tokenized_ids'] = sentence_piece_input['sentence'].apply(lambda x: sp.EncodeAsIds(str(x)))\n",
    "sentence_piece_input.to_csv('ms_marco_tokenised.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3279b",
   "metadata": {},
   "source": [
    "# Output token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5092c",
   "metadata": {},
   "source": [
    "## Run word2vec on tokenised corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821bc86",
   "metadata": {},
   "source": [
    "W2V steps:\n",
    "- generate CBOW table\n",
    "- initialise embedding matrix and linear layer\n",
    "- for each loop:\n",
    "    - grab embedding vectors for context words\n",
    "    - sum into one embedding vector\n",
    "    - multiply by linear layer\n",
    "    - softmax the result\n",
    "    - calc loss against target\n",
    "    - backprop\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9e78f",
   "metadata": {},
   "source": [
    "## CBOW table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from two_tower_datasets import W2VData\n",
    "dataset = W2VData(sentence_piece_input, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860888a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of rows in W2V CBOW data\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a high batch size for the data loader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63006a41",
   "metadata": {},
   "source": [
    "## W2V for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba32c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CBOW\n",
    "vocab_size = sp.GetPieceSize()\n",
    "# Initialise CBOW model (vocab_size x embedding_dim)\n",
    "cbow = CBOW(vocab_size, 50)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac118e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of batches in dataloader\n",
    "print (len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c56417",
   "metadata": {},
   "source": [
    "## Train W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3357a",
   "metadata": {},
   "source": [
    "### Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_cbow\n",
    "# Run CBOW training, to get embedding matrix\n",
    "# This will be passed to two-tower model\n",
    "train_cbow(n_epochs=1, model=cbow, loss_function=loss_function, optimizer=optimizer, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f2d2f",
   "metadata": {},
   "source": [
    "Now we have trained an embedding matrix, via the CBOW method, to give us an (vocab_size, embedding_dim) matrix. We have two options now:\n",
    "1. Use an RNN/LSTM to convert these token embeddings into sentence embeddings, for all of our query and document sentences. Follow this up with a two-tower architecture.\n",
    "2. Skip the sentence embedding step, and use the embedding matrix directly in a two-tower (RNN/LSTM) architecture. \n",
    "\n",
    "I'm leaning towards the latter, because, time constraints, less complex architecture, and possibly improved performance, at the cost of training time (I think).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e46e9",
   "metadata": {},
   "source": [
    "# Token -> sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe66c6e",
   "metadata": {},
   "source": [
    "Skip this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81231d50",
   "metadata": {},
   "source": [
    "# PCA(?) to reduce dimensionality of sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86291d6",
   "metadata": {},
   "source": [
    "Skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67ab75",
   "metadata": {},
   "source": [
    "# Two towers -> trained two tower architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbd7eb",
   "metadata": {},
   "source": [
    "## Create dataset to input to two tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd66be8",
   "metadata": {},
   "source": [
    "At each loop, we're going to need:\n",
    "1. The query\n",
    "2. The sentence \n",
    "3. The document the sentence belongs to \n",
    "4. The label (0 or 1 if Bing returned the doc for the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from two_tower_datasets import two_tower_dataset\n",
    "# Reload MS Marco dataset, to create two-tower dataset\n",
    "# dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"train\")\n",
    "dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"validation\")\n",
    "df_train = pd.DataFrame(dataset)\n",
    "print (len(df_train))\n",
    "result_df = two_tower_dataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062991d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['is_selected'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36dab92",
   "metadata": {},
   "source": [
    "## Tokenise the queries and passage texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ef1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['query'] = result_df['query'].apply(lambda x: sp.EncodeAsIds(str(x)))\n",
    "result_df['passage_text'] = result_df['passage_text'].apply(lambda x: sp.EncodeAsIds(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6e386",
   "metadata": {},
   "source": [
    "## Convert this to a dataset, then a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from two_tower_datasets import TwoTowerData, collate_fn, pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "two_tower_dataset = TwoTowerData(result_df)\n",
    "batch_size = 512\n",
    "two_tower_dataloader = DataLoader(two_tower_dataset, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(two_tower_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fc7c2",
   "metadata": {},
   "source": [
    "# Define two-tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from model import TwoTowerModel, CBOW\n",
    "from loss import contrastive_loss\n",
    "from train import train_two_tower\n",
    "\n",
    "# Load CBOW model\n",
    "embedding_weights = cbow.embeddings.weight.data.detach()\n",
    "# Initialise two-tower model\n",
    "model = TwoTowerModel(embedding_matrix=torch.tensor(embedding_weights), hidden_size=128, output_size=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs_two_tower = 2\n",
    "\n",
    "# Run two-tower training\n",
    "train_two_tower(n_epochs_two_tower, model, contrastive_loss, optimizer, two_tower_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d5aa0",
   "metadata": {},
   "source": [
    "To do: \n",
    "Cast all the queries and sentences into embedding space. \n",
    "Then feed in batches as you've got now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f785673",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from inference import create_offline_sentence_embeddings\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# torch.save(model, 'two_tower.pth')\n",
    "\n",
    "# Test\n",
    "sentences = list(sentence_piece_input['sentence'].values)\n",
    "tokenizer = sp\n",
    "\n",
    "offline_embeddings_dict = create_offline_sentence_embeddings(sentences, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a951e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "converted_dict = {k: [v] if not isinstance(v, list) else v for k, v in offline_embeddings_dict.items()}\n",
    "\n",
    "with open('offline_embeddings_dict.json', 'w') as f:\n",
    "    json.dump(converted_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08856644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import get_query_embedding, compute_similarities\n",
    "\n",
    "query = \"Service Technician Salary. Service Technician average salary is $42,052, median salary is $40,000 with a salary range from $20,000 to $100,000\"\n",
    "query_embedding = get_query_embedding(query, model, tokenizer)\n",
    "similarities = compute_similarities(query_embedding, offline_embeddings_dict, model, tokenizer)\n",
    "\n",
    "# Get top 10 matches (adjust as needed)\n",
    "sorted_indices = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "top_matches = sorted_indices[:10]\n",
    "\n",
    "for i in top_matches:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "('Fromage a Raclette. Raclette is a semi-hard cheese made on both sides of the French and Swiss Alps. Valais Raclette or Fromage a Raclette, as they are traditionally called, are made using ancestral methods with unpasteurised milk of cows grazing on the alpine meadows. While Switzerland supplies 80% of Raclettes, French Raclettes are slightly softer with a smooth and creamy flavour. Raclette is also the name of a Swiss dish where the cheese is melted in front of a fire or a special machine and the melted parts are scraped onto diner’s plates. It is then served with small potatoes, gherkins, pickled onions and air dried meat called Viande des Grison', 0.8666157126426697)\n",
    "('Sutton is an English-language surname of England and Ireland. One origin is from Anglo-Saxon where it is derived from sudh, suth, or suð, and tun referring to the generic placename Southtown. Note that almost every county in England contains one or more placenames bearing the prefix Sutton. The Domesday Book (1086) contains the first recorded spelling of the surname as Ketel de Sudtone; Suttuna also appeared in 1086 in records from Ely, Cambridgeshire. In 1379 tax records, the surname appears as de Sutton (of Southtown). One source refers to the origin as being Anglo-Norman, with the name itself derived as described above, from Anglo-Saxon terms. Related surnames include early variants de Sudtone (1086), Suttuna (1086), de Sutton (1379), and de Sutu', 0.7647449970245361)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
