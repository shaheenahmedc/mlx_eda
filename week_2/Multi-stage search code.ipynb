{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ac2da6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder, load_dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import string\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88a99c",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa745934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms_marco = load_dataset_builder(\"ms_marco\", 'v1.1')\n",
    "dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"train\")\n",
    "df_train = pd.DataFrame(dataset)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72d973",
   "metadata": {},
   "source": [
    "# Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf81818",
   "metadata": {},
   "source": [
    "Steps taken:\n",
    "- pip install sentencepiece\n",
    "- prepare data in required format (csv, new line per sentence)\n",
    "- run sentencepiece on corpus, to generate tokens\n",
    "- run sentencepiece embedding on sentences, to convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47876715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[0]['passages']['passage_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a CSV file for writing\n",
    "with open('output.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # 1. Iterate over the rows of the dataframe\n",
    "    for _, row in df_train.iterrows():\n",
    "        # 2. For each row, access the passages column and then the passage_text key\n",
    "        passage_texts = row['passages']['passage_text']\n",
    "\n",
    "        # 3. Write each string from every list to the CSV file\n",
    "        for text in passage_texts:\n",
    "            writer.writerow([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input = pd.read_csv('output.csv', header =None, names = ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c62de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27017f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input = sentence_piece_input.sample(n = 10000)\n",
    "sentence_piece_input.to_csv('sentence_piece_input_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define parameters for training\n",
    "train_args = {\n",
    "    'input': 'sentence_piece_input_10k.csv',             # Input file\n",
    "    'model_prefix': 'mymodel',        # Prefix for the output model files (.model and .vocab)\n",
    "    'vocab_size': 8000,              # Size of the vocabulary\n",
    "    'character_coverage': 0.9995,     # Character coverage to be considered for the model. Good defaults are: 0.9995 for languages with rich character sets like Japanese or Chinese and 0.9997 for others\n",
    "    'model_type': 'unigram',          # Model type can be 'unigram' (default), 'bpe', 'char', or 'word'\n",
    "    # Add other parameters as needed.\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a15b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the vocab file\n",
    "vocab_file = 'mymodel.vocab'\n",
    "\n",
    "# Read and display the first N lines of the vocab file\n",
    "N = 1000  # Change this value to see more or fewer tokens\n",
    "with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= N:\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('mymodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input['tokenized'] = sentence_piece_input['sentence'].apply(lambda x: sp.EncodeAsPieces(str(x)))\n",
    "sentence_piece_input['tokenized_ids'] = sentence_piece_input['sentence'].apply(lambda x: sp.EncodeAsIds(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input.to_csv('ms_marco_sample_tokenised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a376e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input['sentence'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3279b",
   "metadata": {},
   "source": [
    "# Output token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5092c",
   "metadata": {},
   "source": [
    "## Run word2vec on tokenised corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class W2VData(torch.utils.data.Dataset):\n",
    "#     def __init__(self, corpus, window_size=2):\n",
    "#         self.tokenized_corpus = corpus\n",
    "#         self.data = []\n",
    "#         self.create_data(window_size)\n",
    "#         self.build_vocab()\n",
    "\n",
    "#     def build_vocab(self):\n",
    "#         # Flatten the tokenized corpus and get unique tokens\n",
    "#         tokens = [token for sublist in self.tokenized_corpus for token in sublist]\n",
    "#         self.vocab = set(tokens)\n",
    "#         # Create token to index mapping\n",
    "#         self.token2idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "#         self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
    "\n",
    "#     def create_data(self, window_size):\n",
    "#         for tokens in self.tokenized_corpus:\n",
    "#             for i, target in enumerate(tokens):\n",
    "#                 context = (\n",
    "#                     tokens[max(0, i - window_size) : i]\n",
    "#                     + tokens[i + 1 : i + window_size + 1]\n",
    "#                 )\n",
    "#                 if len(context) != 2 * window_size:\n",
    "#                     continue\n",
    "#                 self.data.append((context, target))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         context, target = self.data[idx]\n",
    "#         # Convert tokens to indices\n",
    "#         context_indices = [self.token2idx[token] for token in context]\n",
    "#         target_index = self.token2idx[target]\n",
    "#         return torch.tensor(context_indices), torch.tensor(target_index)\n",
    "    \n",
    "# class CBOW(torch.nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim):\n",
    "#         super(CBOW, self).__init__()\n",
    "#         self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         embeds = torch.sum(self.embeddings(inputs), dim=1)\n",
    "#         out = self.linear(embeds)\n",
    "#         log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "#         return log_probs\n",
    "    \n",
    "# data = pd.read_csv('ms_marco_tokenised.csv')\n",
    "# ds = W2VData(data)\n",
    "# dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "# for batch in dl:\n",
    "#     print(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# vocab_size = len(ds.tknz.vocab)\n",
    "# lang = model.Language(torch.rand(vocab_size, 50), 7)\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(lang.parameters(), lr=0.001)\n",
    "# torch.save(lang.state_dict(), f\"./lang_epoch_0.pt\")\n",
    "\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     for sentence, target, _ in dl:\n",
    "#         optimizer.zero_grad()\n",
    "#         log_probs = lang(sentence)\n",
    "#         loss = loss_function(log_probs, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(f\"Epoch {epoch+1}/5, Loss: {loss.item()}\")\n",
    "#     torch.save(lang.state_dict(), f\"./lang_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821bc86",
   "metadata": {},
   "source": [
    "W2V steps:\n",
    "- generate CBOW table\n",
    "- initialise embedding matrix and linear layer\n",
    "- for each loop:\n",
    "    - grab embedding vectors for context words\n",
    "    - sum into one embedding vector\n",
    "    - multiply by linear layer\n",
    "    - softmax the result\n",
    "    - calc loss against target\n",
    "    - backprop\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a17f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_input_test = sentence_piece_input[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9e78f",
   "metadata": {},
   "source": [
    "## CBOW table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VData(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        self.corpus = corpus\n",
    "        self.data = []\n",
    "        self.create_data(window_size)\n",
    "\n",
    "    def create_data(self, window_size):\n",
    "        for index, row in self.corpus.iterrows():\n",
    "            print (index)\n",
    "            tokens = row['tokenized_ids']\n",
    "            for i, target in enumerate(tokens):\n",
    "                context = (\n",
    "                    tokens[max(0, i - window_size) : i]\n",
    "                    + tokens[i + 1 : i + window_size + 1]\n",
    "                )\n",
    "                if len(context) != 2 * window_size:\n",
    "                    continue\n",
    "                self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context), torch.tensor(target)\n",
    "\n",
    "dataset = W2VData(sentence_piece_input, 2)\n",
    "\n",
    "\n",
    "# data = []\n",
    "# window_size = 2\n",
    "# for index, row in sentence_piece_input.iterrows():\n",
    "#     print (index)\n",
    "#     tokens = row['tokenized_ids']\n",
    "#     for i, target in enumerate(tokens):\n",
    "#         context = (\n",
    "#             tokens[max(0, i - window_size) : i]\n",
    "#             + tokens[i + 1 : i + window_size + 1]\n",
    "#         )\n",
    "#         if len(context) != 2 * window_size:\n",
    "#             continue\n",
    "#         data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860888a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc04f9",
   "metadata": {},
   "source": [
    "## Initialise embedding and linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = torch.sum(self.embeddings(inputs), dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63006a41",
   "metadata": {},
   "source": [
    "## W2V for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba32c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = sp.GetPieceSize()\n",
    "cbow = CBOW(vocab_size, 50)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac118e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c56417",
   "metadata": {},
   "source": [
    "## Train W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for context, target in tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}/2\", unit=\"batch\"):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = cbow(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/2, Loss: {total_loss}\")\n",
    "    torch.save(cbow.state_dict(), f\"./cbow_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f2d2f",
   "metadata": {},
   "source": [
    "Now we have trained an embedding matrix, via the CBOW method, to give us an (vocab_size, embedding_dim) matrix. We have two options now:\n",
    "1. Use an RNN/LSTM to convert these token embeddings into sentence embeddings, for all of our query and document sentences. Follow this up with a two-tower architecture.\n",
    "2. Skip the sentence embedding step, and use the embedding matrix directly in a two-tower (RNN/LSTM) architecture. \n",
    "\n",
    "I'm leaning towards the latter, because, time constraints, less complex architecture, and possibly improved performance, at the cost of training time (I think).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e46e9",
   "metadata": {},
   "source": [
    "# Token -> sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe66c6e",
   "metadata": {},
   "source": [
    "Skip this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81231d50",
   "metadata": {},
   "source": [
    "# PCA(?) to reduce dimensionality of sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86291d6",
   "metadata": {},
   "source": [
    "Skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67ab75",
   "metadata": {},
   "source": [
    "# Two towers -> trained two tower architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbd7eb",
   "metadata": {},
   "source": [
    "## Create dataset to input to two tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd66be8",
   "metadata": {},
   "source": [
    "At each loop, we're going to need:\n",
    "1. The query\n",
    "2. The sentence \n",
    "3. The document the sentence belongs to \n",
    "4. The label (0 or 1 if Bing returned the doc for the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"ms_marco\", 'v1.1', split=\"train\")\n",
    "df_train = pd.DataFrame(dataset)\n",
    "df_train = df_train.sample(n = 10000)\n",
    "\n",
    "# df_train = df_train.loc[:10,:]\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through rows and expand\n",
    "for index, row in df_train.iterrows():\n",
    "    print (index/len(df_train))\n",
    "    query = row['query']\n",
    "    for is_selected, passage in zip(row['passages']['is_selected'], row['passages']['passage_text']):\n",
    "        new_rows.append({\n",
    "            'query': query,\n",
    "            'is_selected': is_selected,\n",
    "            'passage_text': passage\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame(new_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(result_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36dab92",
   "metadata": {},
   "source": [
    "## Tokenise the queries and passage texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ef1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['query'] = result_df['query'].apply(lambda x: sp.EncodeAsIds(str(x)))\n",
    "result_df['passage_text'] = result_df['passage_text'].apply(lambda x: sp.EncodeAsIds(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829559df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6e386",
   "metadata": {},
   "source": [
    "## Convert this to a dataset, then a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TwoTowerData(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query = self.dataframe.iloc[index]['query']\n",
    "        is_selected = self.dataframe.iloc[index]['is_selected']\n",
    "        passage_text = self.dataframe.iloc[index]['passage_text']\n",
    "        \n",
    "        \n",
    "        return torch.tensor(query), torch.tensor(is_selected), torch.tensor(passage_text)\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "two_tower_dataset = TwoTowerData(result_df)\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, padding_value=0):\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max([s.size(0) for s in sequences])\n",
    "    out_dims = (len(sequences), max_len) + trailing_dims\n",
    "    \n",
    "    padded_sequences = sequences[0].data.new(*out_dims).fill_(padding_value)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        length = sequence.size(0)\n",
    "        padded_sequences[i, :length, ...] = sequence\n",
    "    return padded_sequences\n",
    "\n",
    "def collate_fn(batch):\n",
    "    queries = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    sentences = [item[2] for item in batch]\n",
    "    \n",
    "    return pad_sequence(queries), torch.stack(labels), pad_sequence(sentences)\n",
    "\n",
    "\n",
    "two_tower_dataloader = DataLoader(two_tower_dataset, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(two_tower_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f933bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(two_tower_dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    \n",
    "    print (batch)\n",
    "    \n",
    "    # To prevent printing all batches (if you have many), you can break after a few:\n",
    "    if batch_idx >= 2:  # Change this number as needed\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = DataLoader(two_tower_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "iter_obj = iter(example)\n",
    "next(iter_obj)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tower_dataset.__getitem__(0)[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031090cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tower_dataset.__getitem__(1)[2].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fc7c2",
   "metadata": {},
   "source": [
    "# Define two-tower model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, output_size):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        \n",
    "        # LSTM layers for query and sentence\n",
    "        self.query_lstm = nn.LSTM(embedding_matrix.size(1), hidden_size, batch_first=True)\n",
    "        self.sentence_lstm = nn.LSTM(embedding_matrix.size(1), hidden_size, batch_first=True)\n",
    "        \n",
    "        # Dense layer to produce final embeddings\n",
    "        self.query_dense = nn.Linear(hidden_size, output_size)\n",
    "        self.sentence_dense = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, query, sentence):\n",
    "        query_embed = self.embedding(query)\n",
    "        sentence_embed = self.embedding(sentence)\n",
    "        \n",
    "        _, (query_hidden, _) = self.query_lstm(query_embed)\n",
    "        _, (sentence_hidden, _) = self.sentence_lstm(sentence_embed)\n",
    "        \n",
    "        query_vector = self.query_dense(query_hidden.squeeze(0))\n",
    "        sentence_vector = self.sentence_dense(sentence_hidden.squeeze(0))\n",
    "        \n",
    "        return query_vector, sentence_vector\n",
    "\n",
    "# Loss function\n",
    "def contrastive_loss(query_vector, sentence_vector, label, margin=0.5):\n",
    "    # Cosine similarity\n",
    "    sim = F.cosine_similarity(query_vector, sentence_vector, dim=1)\n",
    "    \n",
    "    # Loss computation\n",
    "    loss = (1 - label) * torch.pow(sim, 2) + label * torch.pow(F.relu(margin - sim), 2)\n",
    "    return loss.mean()\n",
    "\n",
    "# Example use:\n",
    "embedding_weights = cbow.embeddings.weight.data.detach()\n",
    "model = TwoTowerModel(embedding_matrix=torch.tensor(embedding_weights), hidden_size=128, output_size=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Training loop example:\n",
    "for epoch in range(epochs):\n",
    "    i = 1\n",
    "    for batch_query, label, batch_sentence in two_tower_dataloader: # your data loader here\n",
    "        i +=1 \n",
    "        print (i / len(two_tower_dataloader))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        query_vector, sentence_vector = model(batch_query, batch_sentence)\n",
    "        loss = contrastive_loss(query_vector, sentence_vector, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d5aa0",
   "metadata": {},
   "source": [
    "To do: \n",
    "Cast all the queries and sentences into embedding space. \n",
    "Then feed in batches as you've got now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f785673",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming your tokenizer and preprocessing method:\n",
    "def tokenize_and_tensorize(text, tokenizer, max_length=100):\n",
    "    # You might need to adjust this based on your tokenizer and preprocessing\n",
    "    tokens = tokenizer.EncodeAsIds(str(text))\n",
    "    return torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def get_query_embedding(query, model, tokenizer):\n",
    "    query_tensor = tokenize_and_tensorize(query, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        query_embedding, _ = model(query_tensor, query_tensor)  # We're only interested in the query's embedding\n",
    "    return query_embedding\n",
    "\n",
    "def compute_similarities(query_embedding, sentences, model, tokenizer):\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for sentence in sentences:\n",
    "            i+=1\n",
    "            print (i/len(sentences))\n",
    "            sentence_tensor = tokenize_and_tensorize(sentence, tokenizer)\n",
    "            # Using dummy tensor for query since we only want the sentence embedding\n",
    "            _, sentence_embedding = model(sentence_tensor, sentence_tensor)\n",
    "            # Compute cosine similarity\n",
    "            cosine_similarity = F.cosine_similarity(query_embedding, sentence_embedding)\n",
    "            similarities.append(cosine_similarity.item())\n",
    "    return similarities\n",
    "\n",
    "# Ensure your model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Test\n",
    "query = \"The rental fee ranges from $1,600 to $3,000 for reception and includes 5 hours of event time excluding set up and clean up time. The fee to rent the venue for a wedding ceremony ranges from $1,200 to $1,500 with reception rental. Additional hours can be arranged for a fee of $350/hr. Event time varies depending on time of event. Daytime Weddings 10:00AM-4:00PM & Evening Weddings 5:00PM-11:00PM. The rental fee ranges from $1,600 to $3,000 for reception and includes 5 hours of event time excluding set up and clean up time. The fee to rent the venue for a wedding ceremony ranges from $1,200 to $1,500 with reception rental\"\n",
    "sentences = list(sentence_piece_input['sentence'].values)\n",
    "tokenizer = sp\n",
    "\n",
    "query_embedding = get_query_embedding(query, model, tokenizer)\n",
    "similarities = compute_similarities(query_embedding, sentences, model, tokenizer)\n",
    "\n",
    "# Get top 10 matches (adjust as needed)\n",
    "sorted_indices = sorted(range(len(similarities)), key=lambda k: -similarities[k])\n",
    "top_matches = [sentences[i] for i in sorted_indices[:10]]\n",
    "\n",
    "for i in top_matches:\n",
    "    print(i)\n",
    "    print ('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08856644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45f72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
