{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9274a5c8",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a0d4c",
   "metadata": {},
   "source": [
    "To combine [tinystories](https://arxiv.org/abs/2305.07759), [text-to-sql](https://huggingface.co/datasets/b-mc2/sql-create-context), and [textbooks are all you need datasets](https://ar5iv.labs.arxiv.org/html/2306.11644), into one dataset to train an encoder-decoder Transformer model, for text-to-code tasks. All three are on Huggingface, to avoid data ingestion pains for now. \n",
    "\n",
    "To then tokenise this dataset via SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c714228",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabe7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d7e35",
   "metadata": {},
   "source": [
    "# Load three datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ba1f1",
   "metadata": {},
   "source": [
    "Note: login to huggingface-cli on command line to download textbooks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56245ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below on command line if it doesn't work here\n",
    "# Generate token from HF\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull tinystories from HF\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories')\n",
    "\n",
    "text_to_sql = load_dataset('b-mc2/sql-create-context')\n",
    "\n",
    "textbooks_all_you_need = load_dataset('nampdn-ai/tiny-codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks_all_you_need.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d95e4c",
   "metadata": {},
   "source": [
    "# Combine/Prepare for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14946b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_stories_train = tiny_stories['train']\n",
    "text_to_sql_train = text_to_sql['train']\n",
    "textbooks_all_you_need_train = textbooks_all_you_need['train']\n",
    "\n",
    "print (tiny_stories_train.features)\n",
    "print (text_to_sql_train.features)\n",
    "print (textbooks_all_you_need_train.features)\n",
    "print (len(tiny_stories_train))\n",
    "print (len(text_to_sql_train))\n",
    "print (len(textbooks_all_you_need_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83445636",
   "metadata": {},
   "source": [
    "Create mini versions of each dataset for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_stories_train_testing = tiny_stories_train.shuffle().select(range(1000))\n",
    "text_to_sql_train_testing = text_to_sql_train.shuffle().select(range(1000))\n",
    "textbooks_all_you_need_train_testing = textbooks_all_you_need_train.shuffle().select(range(1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbd883",
   "metadata": {},
   "source": [
    "I'm going to have to feed one 'language' of data into the encoder, and the other into the decoder. Languages = (English, code)? Or is it (English, SQL, Python, Java...)?\n",
    "\n",
    "This will be way too complicated a task, let's just use SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filter function\n",
    "def filter_sql_entries(example):\n",
    "    return 'sql' in example['programming_language'].lower()\n",
    "\n",
    "# Apply the filter function\n",
    "textbooks_all_you_need_train_sql = textbooks_all_you_need_train.filter(filter_sql_entries)\n",
    "textbooks_all_you_need_train_testing_sql = textbooks_all_you_need_train_testing.shuffle().filter(filter_sql_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60571972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(textbooks_all_you_need_train_sql))\n",
    "print (len(textbooks_all_you_need_train_testing_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dee0e1",
   "metadata": {},
   "source": [
    "# Store as individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8133d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing of SP Input\n",
    "# # Combine the relevant fields from each dataset into a single text file for SentencePiece training\n",
    "# with open('SP_data_encoder_decoder_testing.txt', 'w', encoding='utf-8') as f:\n",
    "#     for example in tiny_stories_train_testing:\n",
    "#         f.write(example['text'].replace('\\n', '') + '\\n')\n",
    "\n",
    "#     print ('tiny_stories_train done')\n",
    "#     for example in text_to_sql_train_testing:\n",
    "#         f.write(example['context'] + '\\n')\n",
    "#         f.write(example['question'] + '\\n')\n",
    "#         f.write(example['answer'] + '\\n')  # This is typically the target language in translation tasks\n",
    "#     print ('text_to_sql_train done')\n",
    "#     for example in textbooks_all_you_need_train_testing:\n",
    "#         f.write('\\n'.join([example[field] for field in example if field not in ['idx', 'response']]) + '\\n')\n",
    "\n",
    "# #         f.write(' '.join([example[field] for field in example if field not in ['idx', 'response']]) + '\\n')\n",
    "#         f.write(example['response'] + '\\n')  # Include the 'response' field as it is part of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the relevant fields from each dataset into a single text file for SentencePiece training\n",
    "with open('SP_data_encoder_decoder.txt', 'w', encoding='utf-8') as f:\n",
    "    for example in tiny_stories_train:\n",
    "        f.write(example['text'].replace('\\n', '') + '\\n')\n",
    "    print ('tiny_stories_train done')\n",
    "    for example in text_to_sql_train:\n",
    "        f.write(example['context'] + '\\n')\n",
    "        f.write(example['question'] + '\\n')\n",
    "        f.write(example['answer'] + '\\n')  # This is typically the target language in translation tasks\n",
    "    print ('text_to_sql_train done')\n",
    "    for example in textbooks_all_you_need_train:\n",
    "        f.write(example['prompt'] + '\\n')\n",
    "        f.write(example['response'] + '\\n')  # Include the 'response' field as it is part of the target language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f67b22",
   "metadata": {},
   "source": [
    "# Train SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d816b",
   "metadata": {},
   "source": [
    "Initial runs crashing notebook. Restrict rows used. First get number of rows in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    "\n",
    "# Example usage\n",
    "num_rows = count_rows('SP_data_encoder_decoder.txt')  # Replace 'your_file.txt' with your file path\n",
    "print(f\"Number of rows in the file: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define parameters for training\n",
    "train_args = {\n",
    "    'input': 'SP_data_encoder_decoder.txt',             # Input file\n",
    "    'model_prefix': 'SP_encoder_decoder_model',        # Prefix for the output model files (.model and .vocab)\n",
    "    'vocab_size': 32000,              # Size of the vocabulary\n",
    "    'character_coverage': 0.9997,     # Character coverage to be considered for the model. Good defaults are: 0.9995 for languages with rich character sets like Japanese or Chinese and 0.9997 for others\n",
    "    'model_type': 'unigram',        # Model type can be 'unigram' (default), 'bpe', 'char', or 'word'\n",
    "    'input_sentence_size': 1300000,\n",
    "    'shuffle_input_sentence': True,\n",
    "    'pad_id': 0,\n",
    "    'unk_id': 1,\n",
    "    'bos_id': 2,\n",
    "    'eos_id': 3,\n",
    "    'pad_piece': '[PAD]',\n",
    "    'unk_piece': '[UNK]',\n",
    "    'bos_piece': '[BOS]',\n",
    "    'eos_piece': '[EOS]'}\n",
    "\n",
    "# Train the model\n",
    "spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fb5c2",
   "metadata": {},
   "source": [
    "# Check SP trained well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82655b03",
   "metadata": {},
   "source": [
    "The sentencepiece call above will now:\n",
    "1. identify [BOS] tokens as beginning a sentence, same for [EOS] and end of sentence\n",
    "2. Replace unknown tokens with [UNK]/token ids with 1 \n",
    "3. Replace padding tokens with [PAD], and token ids with 0\n",
    "\n",
    "To check:\n",
    "What is the token encoding for a sample of rows in each dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59473ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentencePiece processor and load your model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('SP_encoder_decoder_model.model')  # Replace with your model file\n",
    "\n",
    "# Your input string\n",
    "input_string =     '''\n",
    "    cin >> consentGiven;\n",
    "\n",
    "    // Based on the user's answer, display appropriate instructions\n",
    "    if (consentGiven) {\n",
    "        cout << endl\n",
    "'''\n",
    "\n",
    "input_string_2 =     '''\n",
    "cin >> consentGiven; // Based on the user's answer, display appropriate instructions if (consentGiven) {cout << endl\n",
    "'''\n",
    "\n",
    "# Encode the string into SentencePiece tokens\n",
    "encoded_pieces = sp.EncodeAsPieces(input_string)\n",
    "print(\"Encoded as pieces:\", encoded_pieces)\n",
    "\n",
    "# Alternatively, encode the string into token IDs\n",
    "encoded_ids = sp.EncodeAsIds(input_string)\n",
    "print(\"Encoded as ids:\", encoded_ids)\n",
    "\n",
    "# Encode the string into SentencePiece tokens\n",
    "encoded_pieces_2 = sp.EncodeAsPieces(input_string_2)\n",
    "print(\"Encoded as pieces:\", encoded_pieces_2)\n",
    "\n",
    "# Alternatively, encode the string into token IDs\n",
    "encoded_ids_2 = sp.EncodeAsIds(input_string_2)\n",
    "print(\"Encoded as ids:\", encoded_ids_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08374",
   "metadata": {},
   "source": [
    "Ok, looks alright over the SQL! Will assume it's fine over tinystories too then, not worried about that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19e39c",
   "metadata": {},
   "source": [
    "# Create training datasets for encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583253c0",
   "metadata": {},
   "source": [
    "For this, we'll have:\n",
    "1. separate out the inputs (natural language plus SQL contextual?) and the outputs (resultant SQL?). \n",
    "2. We'll also have to append BOS, EOS and Padding tokens, just as our trained SP model expects. \n",
    "3. We'll have to create datasets and dataloaders objects for the inputs and outputs separately too. \n",
    "4. We'll also need collate functions to pad each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fb2f7",
   "metadata": {},
   "source": [
    "Input dataset:\n",
    "1. Encoder: tiny_stories['text'], Decoder: X \n",
    "2. text_to_sql['question'] + text_to_sql['context'], Decoder: text_to_sql['answer']\n",
    "3. textbooks_all_you_need['prompt']: textbooks['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ff85c",
   "metadata": {},
   "source": [
    "Maybe start with just (2) and (3) for now. In fact let's ignore context from (2) also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec0b19",
   "metadata": {},
   "source": [
    "- Append BOS and EOS tokens to target sequence/decoder input\n",
    "- Do the same for input sequences/encoder input? Definitely for EOS token, probably doesn't matter for BOS for encoder (but the q/k/v matrices will be different size though. So only use EOS token?)\n",
    "- Full target sequence is [BOS, 1,2,3,4, EOS] \n",
    "- decoder input is [BOS, 1,2,3,4]\n",
    "- Decoder will output predictions on [1,2,3,4,EOS], so use [1,2,3,4,EOS] in loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This method should return a single sample at a time\n",
    "        item = self.hf_dataset[idx]\n",
    "        # Process the item (e.g., tokenization, numericalization) as required\n",
    "        # ...\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value\n",
    "\n",
    "def standardize_types(example):\n",
    "    example['prompt'] = str(example['prompt'])\n",
    "    example['response'] = str(example['response'])\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "text_to_sql_train_mapped = text_to_sql_train.map(lambda example: {'prompt': example['question'], \n",
    "                                         'response': example['answer']}, \n",
    "                        remove_columns=['question', 'context', 'answer'])\n",
    "\n",
    "cols_to_keep = {'prompt', 'response'}\n",
    "\n",
    "columns_to_remove = [col for col in textbooks_all_you_need_train_sql.column_names if col not in cols_to_keep]\n",
    "\n",
    "textbooks_all_you_need_train_sql_trimmed = textbooks_all_you_need_train_sql.map(lambda example: example, remove_columns=columns_to_remove)\n",
    "\n",
    "textbooks_all_you_need_train_sql_trimmed_2 = textbooks_all_you_need_train_sql_trimmed.cast(\n",
    "    Features({\"response\": Value(\"string\"), \"prompt\": Value(\"string\")}))\n",
    "combined_dataset = concatenate_datasets([text_to_sql_train_mapped, textbooks_all_you_need_train_sql_trimmed_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(textbooks_all_you_need_train_sql_trimmed))\n",
    "print (len(text_to_sql_train_mapped))\n",
    "print (len(combined_dataset))\n",
    "\n",
    "for i in range(5):\n",
    "    print (combined_dataset[i])\n",
    "    print (combined_dataset[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d54b0f",
   "metadata": {},
   "source": [
    "Looks like dataset concatenation was successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40fa7d",
   "metadata": {},
   "source": [
    "# Create Dataset Class for encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderDataset(t.utils.data.Dataset):\n",
    "    def __init__(self, input_data, sp_tokenizer_model):\n",
    "        self.column_names = ['prompt', 'response']\n",
    "        self.data = input_data\n",
    "        self.tokenizer = sp_tokenizer_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        model_input = str(self.tokenizer.sp.bos_id()) \n",
    "        + ' ' + self.tokenizer.encode(row['[prompt]']) \n",
    "        + ' ' + str(self.tokenizer.sp.eos_id())\n",
    "        \n",
    "        ground_truth = str(self.tokenizer.sp.bos_id()) \n",
    "        + ' ' + self.tokenizer.encode(row['[response]']) \n",
    "        + ' ' + str(self.tokenizer.sp.eos_id())\n",
    "        return {\n",
    "            'model_input': model_input,\n",
    "            'ground_truth': ground_truth,\n",
    "            'tensor_model_input': torch.tensor(model_input),\n",
    "            'tensor_ground_truth': torch.tensor(ground_truth)\n",
    "        }\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        # This should really be using [PAD], but easier to directly add 0s, as 0 is PAD ID from tokenizer\n",
    "        input_pad = t.nn.utils.rnn.pad_sequence([item['model_input'] for item in batch], batch_first=True, padding_value=0)\n",
    "        label_pad = t.nn.utils.rnn.pad_sequence([item['ground_truth'] for item in batch], batch_first=True, padding_value=0)\n",
    "        \n",
    "        return {\n",
    "            'model_inputs': [item['model_input'] for item in batch],\n",
    "            'ground_truths': [item['ground_truth'] for item in batch],\n",
    "            'tensor_model_input': input_pad,\n",
    "            'tensor_ground_truth': label_pad\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cfed2",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderDecoderDataset = EncoderDecoderDataset(\n",
    "    combined_dataset, \n",
    "    sp.load('SP_encoder_decoder_model.model'))\n",
    "\n",
    "EncoderDecoderDataLoader = t.utils.data.DataLoader(\n",
    "    EncoderDecoderDataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=EncoderDecoderDataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da7034",
   "metadata": {},
   "source": [
    "# Create Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d40e4",
   "metadata": {},
   "source": [
    "Adjustments to make, to Transformer Code, to implement encoder-decoder:\n",
    "1. Create separate encoder and decoder transformer classes, replacing the TransformerBlock and DemoTransformer classes. \n",
    "2. Implement cross-attention, after self-attention, in the decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fea09d",
   "metadata": {},
   "source": [
    "## Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "#     debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0706ff",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48c5de",
   "metadata": {},
   "source": [
    "## Embedding Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86772f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty(cfg.d_vocab, cfg.d_model))\n",
    "        nn.init.normal_(self.W_E, std = self.cfg.init_range)\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "    \n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty(cfg.n_ctx, cfg.d_model))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch = batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e719",
   "metadata": {},
   "source": [
    "## Transformer Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29751c2f",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7499db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "    \n",
    "    def __init__(self, cfg:Config, is_causal: bool, decoder = True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg \n",
    "        self.W_Q = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_K = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_V = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_O = nn.Parameter(t.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "        self.key_activations = t.empty(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
    "        self.value_activations = t.empty(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        Keys = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_K,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_K\n",
    "        \n",
    "        if (decoder and is_causal):\n",
    "            Keys = Keys\n",
    "        elif (decoder and not is_causal):\n",
    "            Keys = self.key_activations\n",
    "        elif (not decoder and not is_causal):\n",
    "            self.key_activations = Keys\n",
    "        else:\n",
    "            print ('Error in attention at key activation bools!')\n",
    "\n",
    "        Queries = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_Q,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_Q\n",
    "        \n",
    "        Values = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_V,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_V\n",
    "        \n",
    "        if (decoder and is_causal):\n",
    "            Values = Values\n",
    "        elif (decoder and not is_causal):\n",
    "            Values = self.key_activations\n",
    "        elif (not decoder and not is_causal):\n",
    "            self.value_activations = Values\n",
    "        else:\n",
    "            print ('Error in attention at value activation bools!')\n",
    "                \n",
    "        Attention_Scores = einops.einsum(\n",
    "            Queries,\n",
    "            Keys,\n",
    "            \"batch seq_len_Q n_heads d_head, batch seq_len_K n_heads d_head -> batch n_heads seq_len_Q seq_len_K\")\n",
    "        \n",
    "        # Make Attention dependant on is_causal bool\n",
    "        # for encoder-decoder, which uses both masked and unmasked attention\n",
    "        if (is_causal):\n",
    "            Attention_Scores_Scaled = self.apply_causal_mask(Attention_Scores / self.cfg.d_head**0.5)\n",
    "        else:\n",
    "            Attention_Scores_Scaled = Attention_Scores / self.cfg.d_head**0.5\n",
    "            \n",
    "        Attention_Scores_Scaled_Softmaxed = Attention_Scores_Scaled.softmax(-1)\n",
    "\n",
    "        Z = einops.einsum(\n",
    "            Values,\n",
    "            Attention_Scores_Scaled_Softmaxed,\n",
    "            \"batch seq_len_K n_heads d_head, batch n_heads seq_len_Q seq_len_K -> batch seq_len_Q n_heads d_head\")\n",
    "\n",
    "        Attention_Out = einops.einsum(\n",
    "            Z, \n",
    "            self.W_O, \n",
    "            \"batch seq_len_Q n_heads d_head, n_heads d_head d_model -> batch seq_len_Q d_model\"\n",
    "            ) + self.b_O\n",
    "        \n",
    "        return Attention_Out\n",
    "    \n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        key_by_query_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device = attn_scores.device)\n",
    "        mask = t.triu(key_by_query_ones, diagonal = 1).bool()\n",
    "        attn_scores.masked_fill(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032baa7",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty(cfg.d_model, cfg.d_mlp))\n",
    "        self.W_out = nn.Parameter(t.empty(cfg.d_mlp, cfg.d_model))\n",
    "        self.b_in = nn.Parameter(t.zeros(cfg.d_mlp))\n",
    "        self.b_out = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        nn.init.normal_(self.W_in, std = self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std = self.cfg.init_range)\n",
    "    \n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        \n",
    "        post_W_in = einops.einsum(\n",
    "            normalized_resid_mid,\n",
    "            self.W_in,\n",
    "            \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\") + self.b_in\n",
    "        \n",
    "        post_activation = gelu_new(post_W_in) \n",
    "        \n",
    "        post_W_out = einops.einsum(\n",
    "            post_activation,\n",
    "            self.W_out, \n",
    "            \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\") + self.b_out\n",
    "        return post_W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fb443",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee6a0e",
   "metadata": {},
   "source": [
    "## Assemble Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=True)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        attention\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b43123",
   "metadata": {},
   "source": [
    "## Assemble Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9ca01",
   "metadata": {},
   "source": [
    "In the encoder block, we just need to remove the causal mask, and then output the key and value activations, on a certain input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=False, decoder = False)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "            \n",
    "        attention_activations = self.attn(self.ln1(resid_pre))\n",
    "        resid_mid = attention_activations + resid_pre\n",
    "\n",
    "        mlp_activations = self.mlp(self.ln2(resid_mid))\n",
    "        resid_post = mlp_activations + resid_mid\n",
    "        return resid_post, attention_activations.key_activations, attention_activations.value_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58744fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=True, decoder = True)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.attn2 = Attention(cfg, is_causal=False, decoder = True)\n",
    "        self.ln3 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        resid_pre: Float[Tensor, \"batch seq_len d_model\"],\n",
    "        key_activations: Float[Tensor, \"batch seq_len n_heads d_head\"],\n",
    "        value_activations: Float[Tensor, \"batch seq_len n_heads d_head\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        self.attn2.key_activations = key_activations\n",
    "        self.attn2.value_activations = value_activations\n",
    "        resid_post_causal_attention = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post_cross_attention = self.attn2(self.ln2(resid_post_causal_attention)) + resid_post_causal_attention\n",
    "        resid_post_mlp = self.mlp(self.ln2(resid_post_cross_attention)) + resid_post_cross_attention\n",
    "        return resid_post_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2b9c1",
   "metadata": {},
   "source": [
    "## Unembedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e821da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty(cfg.d_model, cfg.d_vocab))\n",
    "        self.b_U = nn.Parameter(t.zeros(cfg.d_vocab), requires_grad = False)\n",
    "        nn.init.normal_(self.W_U, std = self.cfg.init_range)\n",
    "        \n",
    "    def forward(\n",
    "        self, resid_stream: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        Unembedding = einops.einsum(\n",
    "            resid_stream,\n",
    "            self.W_U,\n",
    "            \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\") + self.b_U\n",
    "        return Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d235d1",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens: Float[Tensor, \"batch seq_len\"]\n",
    "               ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.encoder_blocks:\n",
    "            residual, key_activations, value_activations = block(residual)\n",
    "        for block in self.decoder_blocks:\n",
    "            residual = block(residual, key_activations, value_activations)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transformer = DemoTransformer(Config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca4f8b",
   "metadata": {},
   "source": [
    "# Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702097cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "model = DemoTransformer(model_cfg)\n",
    "\n",
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 16\n",
    "    epochs = 5\n",
    "    max_steps_per_epoch = 100\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "    wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992a83c",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cd773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5e5172b",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46199043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
