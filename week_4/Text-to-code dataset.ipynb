{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9274a5c8",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a0d4c",
   "metadata": {},
   "source": [
    "To combine [tinystories](https://arxiv.org/abs/2305.07759), [text-to-sql](https://huggingface.co/datasets/b-mc2/sql-create-context), and [textbooks are all you need datasets](https://ar5iv.labs.arxiv.org/html/2306.11644), into one dataset to train an encoder-decoder Transformer model, for text-to-code tasks. All three are on Huggingface, to avoid data ingestion pains for now. \n",
    "\n",
    "To then tokenise this dataset via SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c714228",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabe7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "from datasets import concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d7e35",
   "metadata": {},
   "source": [
    "# Load three datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ba1f1",
   "metadata": {},
   "source": [
    "Note: login to huggingface-cli on command line to download textbooks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56245ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below on command line if it doesn't work here\n",
    "# Generate token from HF\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull tinystories from HF\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories')\n",
    "\n",
    "text_to_sql = load_dataset('b-mc2/sql-create-context')\n",
    "\n",
    "textbooks_all_you_need = load_dataset('nampdn-ai/tiny-codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks_all_you_need.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d95e4c",
   "metadata": {},
   "source": [
    "# Combine/Prepare for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14946b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_stories_train = tiny_stories['train']\n",
    "text_to_sql_train = text_to_sql['train']\n",
    "textbooks_all_you_need_train = textbooks_all_you_need['train']\n",
    "\n",
    "print (tiny_stories_train.features)\n",
    "print (text_to_sql_train.features)\n",
    "print (textbooks_all_you_need_train.features)\n",
    "print (len(tiny_stories_train))\n",
    "print (len(text_to_sql_train))\n",
    "print (len(textbooks_all_you_need_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83445636",
   "metadata": {},
   "source": [
    "Create mini versions of each dataset for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_stories_train_testing = tiny_stories_train.shuffle().select(range(1000))\n",
    "text_to_sql_train_testing = text_to_sql_train.shuffle().select(range(1000))\n",
    "textbooks_all_you_need_train_testing = textbooks_all_you_need_train.shuffle().select(range(1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbd883",
   "metadata": {},
   "source": [
    "I'm going to have to feed one 'language' of data into the encoder, and the other into the decoder. Languages = (English, code)? Or is it (English, SQL, Python, Java...)?\n",
    "\n",
    "This will be way too complicated a task, let's just use SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filter function\n",
    "def filter_sql_entries(example):\n",
    "    return 'sql' in example['programming_language'].lower()\n",
    "\n",
    "# Apply the filter function\n",
    "textbooks_all_you_need_train_sql = textbooks_all_you_need_train.filter(filter_sql_entries)\n",
    "textbooks_all_you_need_train_testing_sql = textbooks_all_you_need_train_testing.shuffle().filter(filter_sql_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60571972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(textbooks_all_you_need_train_sql))\n",
    "print (len(textbooks_all_you_need_train_testing_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dee0e1",
   "metadata": {},
   "source": [
    "# Store as individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8133d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing of SP Input\n",
    "# # Combine the relevant fields from each dataset into a single text file for SentencePiece training\n",
    "# with open('SP_data_encoder_decoder_testing.txt', 'w', encoding='utf-8') as f:\n",
    "#     for example in tiny_stories_train_testing:\n",
    "#         f.write(example['text'].replace('\\n', '') + '\\n')\n",
    "\n",
    "#     print ('tiny_stories_train done')\n",
    "#     for example in text_to_sql_train_testing:\n",
    "#         f.write(example['context'] + '\\n')\n",
    "#         f.write(example['question'] + '\\n')\n",
    "#         f.write(example['answer'] + '\\n')  # This is typically the target language in translation tasks\n",
    "#     print ('text_to_sql_train done')\n",
    "#     for example in textbooks_all_you_need_train_testing:\n",
    "#         f.write('\\n'.join([example[field] for field in example if field not in ['idx', 'response']]) + '\\n')\n",
    "\n",
    "# #         f.write(' '.join([example[field] for field in example if field not in ['idx', 'response']]) + '\\n')\n",
    "#         f.write(example['response'] + '\\n')  # Include the 'response' field as it is part of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the relevant fields from each dataset into a single text file for SentencePiece training\n",
    "with open('SP_data_encoder_decoder.txt', 'w', encoding='utf-8') as f:\n",
    "    for example in tiny_stories_train:\n",
    "        f.write(example['text'].replace('\\n', '') + '\\n')\n",
    "    print ('tiny_stories_train done')\n",
    "    for example in text_to_sql_train:\n",
    "        f.write(example['context'] + '\\n')\n",
    "        f.write(example['question'] + '\\n')\n",
    "        f.write(example['answer'] + '\\n')  # This is typically the target language in translation tasks\n",
    "    print ('text_to_sql_train done')\n",
    "    for example in textbooks_all_you_need_train:\n",
    "        f.write(example['prompt'] + '\\n')\n",
    "        f.write(example['response'] + '\\n')  # Include the 'response' field as it is part of the target language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f67b22",
   "metadata": {},
   "source": [
    "# Train SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d816b",
   "metadata": {},
   "source": [
    "Initial runs crashing notebook. Restrict rows used. First get number of rows in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    return line_count\n",
    "\n",
    "# Example usage\n",
    "num_rows = count_rows('SP_data_encoder_decoder.txt')  # Replace 'your_file.txt' with your file path\n",
    "print(f\"Number of rows in the file: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define parameters for training\n",
    "train_args = {\n",
    "    'input': 'SP_data_encoder_decoder.txt',             # Input file\n",
    "    'model_prefix': 'SP_encoder_decoder_model',        # Prefix for the output model files (.model and .vocab)\n",
    "    'vocab_size': 3200,              # Size of the vocabulary\n",
    "    'character_coverage': 0.9997,     # Character coverage to be considered for the model. Good defaults are: 0.9995 for languages with rich character sets like Japanese or Chinese and 0.9997 for others\n",
    "    'model_type': 'unigram',        # Model type can be 'unigram' (default), 'bpe', 'char', or 'word'\n",
    "    'input_sentence_size': 1300000,\n",
    "    'shuffle_input_sentence': True,\n",
    "    'pad_id': 0,\n",
    "    'unk_id': 1,\n",
    "    'bos_id': 2,\n",
    "    'eos_id': 3,\n",
    "    'pad_piece': '[PAD]',\n",
    "    'unk_piece': '[UNK]',\n",
    "    'bos_piece': '[BOS]',\n",
    "    'eos_piece': '[EOS]'}\n",
    "\n",
    "# Train the model\n",
    "spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fb5c2",
   "metadata": {},
   "source": [
    "# Check SP trained well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82655b03",
   "metadata": {},
   "source": [
    "The sentencepiece call above will now:\n",
    "1. identify [BOS] tokens as beginning a sentence, same for [EOS] and end of sentence\n",
    "2. Replace unknown tokens with [UNK]/token ids with 1 \n",
    "3. Replace padding tokens with [PAD], and token ids with 0\n",
    "\n",
    "To check:\n",
    "What is the token encoding for a sample of rows in each dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59473ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Initialize SentencePiece processor and load your model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('SP_encoder_decoder_model.model')  # Replace with your model file\n",
    "\n",
    "# Your input string\n",
    "input_string =     '''\n",
    "    cin >> consentGiven;\n",
    "\n",
    "    // Based on the user's answer, display appropriate instructions\n",
    "    if (consentGiven) {\n",
    "        cout << endl\n",
    "'''\n",
    "\n",
    "input_string_2 =     '''\n",
    "cin >> consentGiven; // Based on the user's answer, display appropriate instructions if (consentGiven) {cout << endl\n",
    "'''\n",
    "\n",
    "# Encode the string into SentencePiece tokens\n",
    "encoded_pieces = sp.EncodeAsPieces(input_string)\n",
    "print(\"Encoded as pieces:\", encoded_pieces)\n",
    "\n",
    "# Alternatively, encode the string into token IDs\n",
    "encoded_ids = sp.EncodeAsIds(input_string)\n",
    "print(\"Encoded as ids:\", encoded_ids)\n",
    "\n",
    "# Encode the string into SentencePiece tokens\n",
    "encoded_pieces_2 = sp.EncodeAsPieces(input_string_2)\n",
    "print(\"Encoded as pieces:\", encoded_pieces_2)\n",
    "\n",
    "# Alternatively, encode the string into token IDs\n",
    "encoded_ids_2 = sp.EncodeAsIds(input_string_2)\n",
    "print(\"Encoded as ids:\", encoded_ids_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08374",
   "metadata": {},
   "source": [
    "Ok, looks alright over the SQL! Will assume it's fine over tinystories too then, not worried about that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19e39c",
   "metadata": {},
   "source": [
    "# Create training datasets for encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583253c0",
   "metadata": {},
   "source": [
    "For this, we'll have:\n",
    "1. separate out the inputs (natural language plus SQL contextual?) and the outputs (resultant SQL?). \n",
    "2. We'll also have to append BOS, EOS and Padding tokens, just as our trained SP model expects. \n",
    "3. We'll have to create datasets and dataloaders objects for the inputs and outputs separately too. \n",
    "4. We'll also need collate functions to pad each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fb2f7",
   "metadata": {},
   "source": [
    "Input dataset:\n",
    "1. Encoder: tiny_stories['text'], Decoder: X \n",
    "2. text_to_sql['question'] + text_to_sql['context'], Decoder: text_to_sql['answer']\n",
    "3. textbooks_all_you_need['prompt']: textbooks['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ff85c",
   "metadata": {},
   "source": [
    "Maybe start with just (2) and (3) for now. In fact let's ignore context from (2) also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec0b19",
   "metadata": {},
   "source": [
    "- Append BOS and EOS tokens to target sequence/decoder input\n",
    "- Do the same for input sequences/encoder input? Definitely for EOS token, probably doesn't matter for BOS for encoder (but the q/k/v matrices will be different size though. So only use EOS token?)\n",
    "- Full target sequence is [BOS, 1,2,3,4, EOS] \n",
    "- decoder input is [BOS, 1,2,3,4]\n",
    "- Decoder will output predictions on [1,2,3,4,EOS], so use [1,2,3,4,EOS] in loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderDecoderDataset(t.utils.data.Dataset):\n",
    "#     def __init__(self, hf_dataset):\n",
    "#         self.hf_dataset = hf_dataset\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.hf_dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # This method should return a single sample at a time\n",
    "#         item = self.hf_dataset[idx]\n",
    "#         # Process the item (e.g., tokenization, numericalization) as required\n",
    "#         # ...\n",
    "#         return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value\n",
    "\n",
    "def standardize_types(example):\n",
    "    example['prompt'] = str(example['prompt'])\n",
    "    example['response'] = str(example['response'])\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "text_to_sql_train_mapped = text_to_sql_train.map(lambda example: {'prompt': example['question'], \n",
    "                                         'response': example['answer']}, \n",
    "                        remove_columns=['question', 'context', 'answer'])\n",
    "\n",
    "cols_to_keep = {'prompt', 'response'}\n",
    "\n",
    "columns_to_remove = [col for col in textbooks_all_you_need_train_sql.column_names if col not in cols_to_keep]\n",
    "\n",
    "textbooks_all_you_need_train_sql_trimmed = textbooks_all_you_need_train_sql.map(lambda example: example, remove_columns=columns_to_remove)\n",
    "\n",
    "textbooks_all_you_need_train_sql_trimmed_2 = textbooks_all_you_need_train_sql_trimmed.cast(\n",
    "    Features({\"response\": Value(\"string\"), \"prompt\": Value(\"string\")}))\n",
    "combined_dataset = concatenate_datasets([text_to_sql_train_mapped, textbooks_all_you_need_train_sql_trimmed_2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9047fe-39e5-4fe7-8f8f-339c61fba86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(textbooks_all_you_need_train_sql_trimmed))\n",
    "print (len(text_to_sql_train_mapped))\n",
    "print (len(combined_dataset))\n",
    "\n",
    "for i in range(5):\n",
    "    print (combined_dataset[i])\n",
    "    print (combined_dataset[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d54b0f",
   "metadata": {},
   "source": [
    "Looks like dataset concatenation was successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40fa7d",
   "metadata": {},
   "source": [
    "# Create Dataset Class for encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "from dataset import EncoderDecoderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835cdc1-7490-44c3-9880-a565df7c635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config\n",
    "import importlib\n",
    "importlib.reload(Config)\n",
    "from Config import Config\n",
    "\n",
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 32000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b3e9e-612f-48ff-927e-74f9258d424c",
   "metadata": {},
   "source": [
    "# Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a1716-0451-4488-8011-929ba9d23059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderDecoderData = EncoderDecoderDataset(\n",
    "#     combined_dataset, \n",
    "#     sp)\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# # Assuming `my_dataset` is your dataset instance\n",
    "# dataset_size = len(EncoderDecoderData)\n",
    "\n",
    "# # Set the percentage for the test data (e.g., 20%)\n",
    "# test_pct = 0.20  # 20% of the dataset\n",
    "# test_size = int(dataset_size * test_pct)\n",
    "# train_size = dataset_size - test_size\n",
    "\n",
    "# # Split the dataset\n",
    "# EncoderDecoderData_train, EncoderDecoderData_test = random_split(EncoderDecoderData, [train_size, test_size])\n",
    "\n",
    "# Create two separate EncoderDecoderDataset instances with random sampling\n",
    "num_samples = len(combined_dataset)\n",
    "train_ratio = 0.8  # Adjust this ratio as needed\n",
    "\n",
    "train_indices = np.random.choice(num_samples, int(train_ratio * num_samples), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(num_samples), train_indices)\n",
    "\n",
    "EncoderDecoderData_train = EncoderDecoderDataset(combined_dataset.select(train_indices), sp, model_cfg)\n",
    "EncoderDecoderData_test = EncoderDecoderDataset(combined_dataset.select(test_indices), sp, model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e96b2-f784-4534-a13b-2310322506c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sp\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cfed2",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     EncoderDecoderDataset_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=False,\n",
    "#     collate_fn = EncoderDecoderDataset.collate_fn\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     EncoderDecoderDataset_test,\n",
    "#     batch_size=args.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=False,\n",
    "#     collate_fn = EncoderDecoderDataset.collate_fn\n",
    "# )\n",
    "\n",
    "# EncoderDecoderDataLoader = t.utils.data.DataLoader(\n",
    "#     EncoderDecoderDataset,\n",
    "#     batch_size=64,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=EncoderDecoderDataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e07f3-4dc4-4891-8baf-d654d32f7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(EncoderDecoderData_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c3b70-f173-48ea-8ad7-0de9601605dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(EncoderDecoderData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be1211-84c2-4740-9d01-0fb4c17e2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94079260-96a6-4465-998d-9eef43fdafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da7034",
   "metadata": {},
   "source": [
    "# Create Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d40e4",
   "metadata": {},
   "source": [
    "Adjustments to make, to Transformer Code, to implement encoder-decoder:\n",
    "1. Create separate encoder and decoder transformer classes, replacing the TransformerBlock and DemoTransformer classes. \n",
    "2. Implement cross-attention, after self-attention, in the decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fea09d",
   "metadata": {},
   "source": [
    "## Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Config:\n",
    "#     d_model: int = 768\n",
    "# #     debug: bool = True\n",
    "#     layer_norm_eps: float = 1e-5\n",
    "#     d_vocab: int = 50257\n",
    "#     init_range: float = 0.02\n",
    "#     n_ctx: int = 1024\n",
    "#     d_head: int = 64\n",
    "#     d_mlp: int = 3072\n",
    "#     n_heads: int = 12\n",
    "#     n_layers: int = 12\n",
    "#     device: str = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# cfg = Config()\n",
    "# print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0706ff",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48c5de",
   "metadata": {},
   "source": [
    "## Embedding Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86772f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty(cfg.d_vocab, cfg.d_model))\n",
    "        nn.init.normal_(self.W_E, std = self.cfg.init_range)\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "    \n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty(cfg.n_ctx, cfg.d_model))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch = batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e719",
   "metadata": {},
   "source": [
    "## Transformer Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29751c2f",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7499db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "    \n",
    "    def __init__(self, cfg:Config, is_causal: bool):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg \n",
    "        self.is_causal = is_causal\n",
    "        self.W_Q = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_K = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_V = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_O = nn.Parameter(t.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.device = cfg.device\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=self.device))\n",
    "        # self.key_activations = t.empty(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
    "        # self.value_activations = t.empty(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        normalized_resid_pre: Float[Tensor, \"batch seq_len d_model\"],\n",
    "        key_activations: Float[Tensor, \"batch seq_len n_heads d_head\"] = None, \n",
    "        value_activations: Float[Tensor, \"batch seq_len n_heads d_head\"] = None, \n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "\n",
    "        Queries = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_Q,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_Q\n",
    "\n",
    "        if key_activations is None:\n",
    "            Keys = einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "                \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "                ) + self.b_K\n",
    "            # self.key_activations = Keys\n",
    "        else:\n",
    "            Keys = key_activations\n",
    "        \n",
    "\n",
    "        if value_activations is None:\n",
    "            Values = einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "                \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "                ) + self.b_V\n",
    "            # self.value_activations = Values\n",
    "        else:\n",
    "            Values = value_activations\n",
    "                \n",
    "        Attention_Scores = einops.einsum(\n",
    "            Queries,\n",
    "            Keys,\n",
    "            \"batch seq_len_Q n_heads d_head, batch seq_len_K n_heads d_head -> batch n_heads seq_len_Q seq_len_K\")\n",
    "        \n",
    "        # Only apply causal_attention if in decoder, via is_causal bool\n",
    "        if (self.is_causal):\n",
    "            Attention_Scores = self.apply_causal_mask(Attention_Scores)\n",
    "\n",
    "        Attention_Scores_Scaled = Attention_Scores / (self.cfg.d_head**0.5)\n",
    "        Attention_Scores_Scaled_Softmaxed = Attention_Scores_Scaled.softmax(-1)\n",
    "\n",
    "        Z = einops.einsum(\n",
    "            Values,\n",
    "            Attention_Scores_Scaled_Softmaxed,\n",
    "            \"batch seq_len_K n_heads d_head, batch n_heads seq_len_Q seq_len_K -> batch seq_len_Q n_heads d_head\")\n",
    "\n",
    "        Attention_Out = einops.einsum(\n",
    "            Z, \n",
    "            self.W_O, \n",
    "            \"batch seq_len_Q n_heads d_head, n_heads d_head d_model -> batch seq_len_Q d_model\"\n",
    "            ) + self.b_O\n",
    "\n",
    "        if (self.is_causal):\n",
    "            return Attention_Out\n",
    "        else:\n",
    "            return Attention_Out, Keys, Values\n",
    "    \n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        key_by_query_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device = self.device)\n",
    "        mask = t.triu(key_by_query_ones, diagonal = 1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032baa7",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty(cfg.d_model, cfg.d_mlp))\n",
    "        self.W_out = nn.Parameter(t.empty(cfg.d_mlp, cfg.d_model))\n",
    "        self.b_in = nn.Parameter(t.zeros(cfg.d_mlp))\n",
    "        self.b_out = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        nn.init.normal_(self.W_in, std = self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std = self.cfg.init_range)\n",
    "    \n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        \n",
    "        post_W_in = einops.einsum(\n",
    "            normalized_resid_mid,\n",
    "            self.W_in,\n",
    "            \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\") + self.b_in\n",
    "        \n",
    "        post_activation = gelu_new(post_W_in) \n",
    "        \n",
    "        post_W_out = einops.einsum(\n",
    "            post_activation,\n",
    "            self.W_out, \n",
    "            \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\") + self.b_out\n",
    "        return post_W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fb443",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee6a0e",
   "metadata": {},
   "source": [
    "## Assemble Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=True)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        attention\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b43123",
   "metadata": {},
   "source": [
    "## Assemble Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9ca01",
   "metadata": {},
   "source": [
    "In the encoder block, we just need to remove the causal mask, and then output the key and value activations, on a certain input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=False)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "            \n",
    "        attention_activations, key_activations, value_activations = self.attn(self.ln1(resid_pre))\n",
    "        resid_mid = attention_activations + resid_pre\n",
    "\n",
    "        mlp_activations = self.mlp(self.ln2(resid_mid))\n",
    "        resid_post = mlp_activations + resid_mid\n",
    "        return resid_post, key_activations, value_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58744fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg, is_causal=True)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.attn2 = Attention(cfg, is_causal=False)\n",
    "        self.ln3 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        resid_pre: Float[Tensor, \"batch seq_len d_model\"],\n",
    "        key_activations: Float[Tensor, \"batch seq_len n_heads d_head\"],\n",
    "        value_activations: Float[Tensor, \"batch seq_len n_heads d_head\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # self.attn2.key_activations = key_activations\n",
    "        # self.attn2.value_activations = value_activations\n",
    "        resid_post_causal_attention = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        \n",
    "        # activations_post_cross_attention, _, _ = self.attn2(self.ln2(resid_post_causal_attention))\n",
    "        activations_post_cross_attention, _, _ = self.attn2(self.ln2(resid_post_causal_attention), key_activations, value_activations)\n",
    "        resid_post_cross_attention = activations_post_cross_attention + resid_post_causal_attention\n",
    "\n",
    "        # resid_post_cross_attention, = self.attn2(self.ln2(resid_post_causal_attention))[0] + resid_post_causal_attention\n",
    "        resid_post_mlp = self.mlp(self.ln2(resid_post_cross_attention)) + resid_post_cross_attention\n",
    "        return resid_post_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2b9c1",
   "metadata": {},
   "source": [
    "## Unembedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e821da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty(cfg.d_model, cfg.d_vocab))\n",
    "        self.b_U = nn.Parameter(t.zeros(cfg.d_vocab), requires_grad = False)\n",
    "        nn.init.normal_(self.W_U, std = self.cfg.init_range)\n",
    "        \n",
    "    def forward(\n",
    "        self, resid_stream: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        Unembedding = einops.einsum(\n",
    "            resid_stream,\n",
    "            self.W_U,\n",
    "            \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\") + self.b_U\n",
    "        return Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d235d1",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self,\n",
    "                encoder_input: Float[Tensor, \"batch seq_len\"],\n",
    "                decoder_target: Float[Tensor, \"batch seq_len\"],\n",
    "               ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        # residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        # for block in self.encoder_blocks:\n",
    "        #     residual, key_activations, value_activations = block(residual)\n",
    "        # for block in self.decoder_blocks:\n",
    "        #     residual = block(residual, key_activations, value_activations)\n",
    "        # logits = self.unembed(self.ln_final(residual))\n",
    "        \n",
    "        # Encoding input sequence\n",
    "        encoder_residual = self.embed(encoder_input) + self.pos_embed(encoder_input)\n",
    "        for block in self.encoder_blocks:\n",
    "            encoder_residual, key_activations, value_activations = block(encoder_residual)\n",
    "        # Decoding target sequence\n",
    "        decoder_residual = self.embed(decoder_target) + self.pos_embed(decoder_target)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            decoder_residual = block(decoder_residual, key_activations, value_activations)\n",
    "        logits = self.unembed(self.ln_final(decoder_residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transformer = DemoTransformer(Config).to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca4f8b",
   "metadata": {},
   "source": [
    "# Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702097cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DemoTransformer(model_cfg)\n",
    "\n",
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 4\n",
    "    epochs = 2\n",
    "    max_steps_per_epoch = 3\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "    wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992a83c",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982837d6-691b-49b3-8ab7-40312a4ad6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5172b",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46199043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self,\n",
    "                 args: TransformerTrainingArgs,\n",
    "                 model: DemoTransformer,\n",
    "                 train_data: t.utils.data.Dataset,\n",
    "                 test_data: t.utils.data.Dataset,\n",
    "                 model_cfg: Config,\n",
    "                 loss_fn):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "        self.model_cfg = model_cfg\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def training_step(self, batch: Dict[str, t.Tensor]) -> Float[Tensor, \"\"]:\n",
    "        # Assuming batch contains 'tensor_model_input' and 'tensor_ground_truth' tensors\n",
    "        encoder_input = batch['tensor_model_input'].to(self.model_cfg.device)\n",
    "        decoder_target = batch['tensor_ground_truth'].to(self.model_cfg.device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = self.model(encoder_input, decoder_target)\n",
    "\n",
    "        # Shift target tokens for loss calculation (ignore the last token)\n",
    "        shifted_target_tokens = decoder_target[:, :-1]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = -self.loss_fn(logits[:, :-1], shifted_target_tokens).mean()\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Zero out gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "        \n",
    "        # wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, t.Tensor]):\n",
    "        encoder_input = batch[\"tensor_model_input\"].to(self.model_cfg.device)\n",
    "        decoder_target = batch[\"tensor_ground_truth\"].to(self.model_cfg.device)\n",
    "\n",
    "        logits: Tensor = self.model(encoder_input, decoder_target)[:, :-1]\n",
    "        predicted_tokens = logits.argmax(dim=-1)\n",
    "        \n",
    "        # Shift target tokens for accuracy calculation (ignore the first token)\n",
    "        shifted_target_tokens = decoder_target[:, 1:]\n",
    "        correct_predictions = (predicted_tokens == shifted_target_tokens).flatten()\n",
    "        return correct_predictions\n",
    "\n",
    "    def train(self):\n",
    "        print ('wandb init below')\n",
    "\n",
    "        # wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        print ('wandb init done')\n",
    "\n",
    "        validation_accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total = self.args.max_steps_per_epoch * self.args.epochs)\n",
    "        print ('progress bar made')\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader()):\n",
    "                print ('train started')\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {validation_accuracy:.2f}\")\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "            validation_accuracy = 0\n",
    "            n_batches_for_testing = 0\n",
    "            for batch in self.test_loader():\n",
    "                print ('validation started')\n",
    "                validation_batch_lim = 1\n",
    "                # correct_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
    "                # accuracy = correct_predictions.float().mean().item()\n",
    "\n",
    "                correct_predictions = self.validation_step(batch)\n",
    "                accuracy = correct_predictions.float().mean().item()\n",
    "                validation_accuracy += accuracy \n",
    "                n_batches_for_testing += 1 \n",
    "                if n_batches_for_testing > validation_batch_lim:\n",
    "                    break\n",
    "            validation_accuracy /= n_batches_for_testing\n",
    "                \n",
    "            # wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "        # wandb.finish()\n",
    "\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_data,\n",
    "                          batch_size=self.args.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn = EncoderDecoderData_train.collate_fn)\n",
    "\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_data,\n",
    "                          batch_size=self.args.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn = EncoderDecoderData_test.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2f998-b2eb-46fc-a7e7-d8460a71f1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbd3dd-d5b4-49e5-a199-54ddd65ee87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TransformerTrainer(args,\n",
    "                             model,\n",
    "                             EncoderDecoderData_train,\n",
    "                             EncoderDecoderData_test,\n",
    "                             model_cfg,\n",
    "                             get_log_probs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e321ef-ed08-4f7f-bc9e-7e64fd3c67fa",
   "metadata": {},
   "source": [
    "Epoch 1: Accuracy = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e5a56-87e0-4386-9246-f1a51cdb32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(), 'encoder_decoder_run.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee023e99-ad28-46dd-8456-6640c079dd4e",
   "metadata": {},
   "source": [
    "# Generate SQL from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a222c-6401-4a77-9e1b-e63883bad40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_model = DemoTransformer(model_cfg)  # Initialize your encoder-decoder model\n",
    "sampling_model.load_state_dict(t.load(\"encoder_decoder_run.pth\"))  # Load pre-trained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d232f-3e52-4138-9321-6be533c8007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderSampler:\n",
    "    def __init__(self, model: DemoTransformer, sp_tokenizer):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = sp_tokenizer\n",
    "        # self.model_cfg = model_cfg\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def generate_text(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates text autoregressively using the encoder-decoder model.\n",
    "\n",
    "        Args:\n",
    "        - prompt (str): The initial input text.\n",
    "        - max_tokens_generated (int): The maximum number of tokens to generate.\n",
    "        - verbose (bool): If True, print the generated text at each step.\n",
    "        - **kwargs: Additional arguments for sampling.\n",
    "\n",
    "        Returns:\n",
    "        - generated_text (str): The autoregressively generated text.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        # max_tokens_generated = self.cfg.n_ctx\n",
    "        # Define the shape for the generated_tokens tensor (batch_size, seq_len)\n",
    "        batch_size = 1  # Replace with your batch size\n",
    "        seq_len = self.cfg.n_ctx    # Replace with your sequence length\n",
    "        \n",
    "        # Encode the input prompt using the encoder\n",
    "        input_ids = self.tokenizer.encode_as_ids(prompt)\n",
    "        encoder_input_ids = t.tensor([input_ids], dtype = t.long).to(self.cfg.device)\n",
    "\n",
    "        decoder_input_ids = t.tensor([[sp.bos_id()]], dtype = t.long).to(self.cfg.device)\n",
    "        # encoder_input_ids_forward_pass = t.zeros((batch_size, seq_len), dtype=t.long).to(self.model_cfg.device)\n",
    "        # encoder_input_ids_forward_pass[0, 0:encoder_input_ids.shape[0]] = encoder_input_ids\n",
    "\n",
    "        # Initialize the generated_tokens tensor with zeros\n",
    "        # generated_tokens = t.zeros((None, seq_len), dtype=t.long).to(self.model_cfg.device)\n",
    "        # generated_tokens[None, 0] = sp.bos_id()\n",
    "        # insertion_point_to_generated_tokens = 1\n",
    "        \n",
    "        for i in range(max_tokens_generated - 1):\n",
    "            # Decode the tokens autoregressively using the decoder\n",
    "            logits = self.model(encoder_input_ids, decoder_input_ids)\n",
    "\n",
    "            # Get the logits for the next token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Sample the next token using a sampling strategy\n",
    "            next_token = self.sample_next_token(decoder_input_ids.squeeze(0), next_token_logits.squeeze(), **kwargs)\n",
    "            next_token_tensor = t.tensor([[next_token]], dtype = decoder_input_ids.dtype, device = decoder_input_ids.device)\n",
    "\n",
    "            # Concatenate the next token to the generated tokens\n",
    "            # generated_tokens = t.cat([generated_tokens, next_token.unsqueeze(1)], dim=-1)\n",
    "            # generated_tokens = t.cat([generated_tokens, t.tensor([next_token], dtype=t.long).unsqueeze(1)], dim=-1)\n",
    "            # generated_tokens[0, insertion_point_to_generated_tokens] = next_token\n",
    "            # print (f'next_token = {next_token}')\n",
    "            # insertion_point_to_generated_tokens += 1\n",
    "            \n",
    "            decoder_input_ids = t.cat((decoder_input_ids, next_token_tensor), dim = 1)\n",
    "\n",
    "            # Print out results, if required\n",
    "            if verbose:\n",
    "                generated_text = self.tokenizer.decode(generated_tokens[0])\n",
    "                print(generated_text, end=\"\\r\")\n",
    "\n",
    "            # Check if the generated text ends with an end-of-sequence token\n",
    "            # if next_token.item() == getattr(self.tokenizer, \"eos_token_id\", None):\n",
    "            if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
    "                break\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        print (f'decoder_input_ids = {decoder_input_ids}')\n",
    "        print (f'decoder_input_ids[0] = {decoder_input_ids[0]}')\n",
    "        \n",
    "        # Convert the generated tokens tensor to a Python list of integers\n",
    "        decoder_input_ids_list = decoder_input_ids[0].tolist()\n",
    "        print (f'decoder_input_ids[0].tolist() = {decoder_input_ids_list}')\n",
    "\n",
    "        # Decode the list of integers using the tokenizer\n",
    "        print (self.tokenizer.get_piece_size())\n",
    "        generated_text = self.tokenizer.decode(decoder_input_ids_list)\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"seq_len d_vocab\"],\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        seed=None,\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (\n",
    "            top_p != 0 and top_k != 0\n",
    "        ), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return EncoderDecoderSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = EncoderDecoderSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = EncoderDecoderSampler.apply_frequency_penalty(\n",
    "                input_ids, logits, frequency_penalty\n",
    "            )\n",
    "        if top_k > 0:\n",
    "            return EncoderDecoderSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return EncoderDecoderSampler.sample_top_p(logits, top_p)\n",
    "        return EncoderDecoderSampler.sample_basic(logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the most likely token (as an int).\n",
    "        \"\"\"\n",
    "        out = logits.argmax().item()\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(\n",
    "        logits: Float[Tensor, \"d_vocab\"], temperature: float\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies temperature scaling to the logits.\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        return logits / temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        freq_penalty: float,\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies a frequency penalty to the logits.\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        d_vocab = logits.size(0)\n",
    "        id_freqs = t.bincount(input_ids, minlength=d_vocab)\n",
    "        return logits - freq_penalty * id_freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the distribution defined by the logits.\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        sampled_token = t.distributions.categorical.Categorical(logits=logits).sample()\n",
    "        return sampled_token.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the top k most likely tokens.\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        top_k_logits, top_k_token_ids = logits.topk(k)\n",
    "        # Get sampled token (which is an index corresponding to the list of top-k tokens)\n",
    "        sampled_token_idx = t.distributions.categorical.Categorical(\n",
    "            logits=top_k_logits\n",
    "        ).sample()\n",
    "        # Get the actual token id, as an int\n",
    "        return top_k_token_ids[sampled_token_idx].item()\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(\n",
    "        logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        # Sort logits, and get cumulative probabilities\n",
    "        logits_sorted, indices = logits.sort(descending=True, stable=True)\n",
    "        cumul_probs = logits_sorted.softmax(-1).cumsum(-1)\n",
    "        # Choose which tokens to keep, in the set we sample from\n",
    "        n_keep = t.searchsorted(cumul_probs, top_p, side=\"left\").item() + 1\n",
    "        n_keep = max(n_keep, min_tokens_to_keep)\n",
    "        keep_idx = indices[:n_keep]\n",
    "        keep_logits = logits[keep_idx]\n",
    "        # Perform the sampling\n",
    "        sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()\n",
    "        return keep_idx[sample].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4edee4-eb24-46ec-b6c6-a9265fc125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = EncoderDecoderSampler(sampling_model, sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e9afd-6724-4cce-9c5c-7b638a3caf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Select all the entries in the person column'\n",
    "\n",
    "sample = sampler.generate_text(prompt = prompt, max_tokens_generated = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afa83f-e49b-4d81-a7a3-6d49f9ed7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8be33-1b3c-48a4-805e-99356d25a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b6c32-7826-447c-9aaa-ee56f1d40ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
