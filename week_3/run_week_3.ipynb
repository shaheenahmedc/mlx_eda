{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook runs through the week 3 task from the MLX apprenticeship, namely re-implementing GPT-2 from scratch.\n",
    "It follows the tutorial [here](https://colab.research.google.com/drive/1Zl3zSdli_epSfaoQ_HeBCuE6dkGWTowd)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Initialise Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Config\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Initialise Demo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DemoTransformer\n",
    "demo_transformer = DemoTransformer(Config).to(cfg.device)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Some Sanity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "demo_transformer.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(cfg.device)\n",
    "demo_logits = demo_transformer(tokens)\n",
    "\n",
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_transformer.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")\n",
    "\n",
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(cfg.device)\n",
    "    demo_logits = demo_transformer(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Create smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "model = DemoTransformer(model_cfg)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Initialise Training Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TransformerTrainingArgs\n",
    "args = TransformerTrainingArgs()\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Prep Dataset and Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories',split='train')\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(tiny_stories,\n",
    "                                            reference_gpt2.tokenizer,\n",
    "                                            streaming=False,\n",
    "                                            max_length=model.cfg.n_ctx,\n",
    "                                            column_name=\"text\",\n",
    "                                            add_bos_token=True,\n",
    "                                            num_proc=10)\n",
    "\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset.save_to_disk('tokenized_tinystories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = train_loader.dataset[:args.batch_size]\n",
    "print(first_batch.keys())\n",
    "print(first_batch['tokens'].shape)\n",
    "print (first_batch)\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Initialise Training Loop Function and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import train\n",
    "importlib.reload(train)\n",
    "from train import TransformerTrainer\n",
    "t.cuda.empty_cache()\n",
    "model = DemoTransformer(model_cfg).to(cfg.device)\n",
    "args = TransformerTrainingArgs()\n",
    "print (args.lr)\n",
    "args.max_steps_per_epoch = 10^9\n",
    "args.batch_size = 64\n",
    "args.epochs = 3\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "args.wandb_project = \"demo_gpt2_may_24\"\n",
    "print (args.max_steps_per_epoch)\n",
    "trainer = TransformerTrainer(args,\n",
    "                             model,\n",
    "                             dataset_dict,\n",
    "                             cfg,\n",
    "                             get_log_probs)\n",
    "trainer.train()\n",
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory Allocated: {t.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory Reserved: {t.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(), 'gpt2_style_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Test Output Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerSampler\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "\n",
    "\n",
    "sampling_model = DemoTransformer(model_cfg).to(cfg.device)\n",
    "sampling_model.load_state_dict(t.load(\"gpt2_style_model_weights.pth\"))\n",
    "\n",
    "sampler = TransformerSampler(sampling_model, tokenizer, model_cfg)\n",
    "\n",
    "prompt = 'Harry and Sally went to the mall '\n",
    "sampler.sample(prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
