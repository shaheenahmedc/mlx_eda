{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca37630-1042-44a9-b4c3-3cab66cd8881",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046333b-f8ae-4558-bbb5-3b88ab6ce7c8",
   "metadata": {},
   "source": [
    "This notebook runs through the week 3 task from the MLX apprenticeship, namely re-implementing GPT-2 from scratch.\n",
    "It follows the tutorial [here](https://colab.research.google.com/drive/1Zl3zSdli_epSfaoQ_HeBCuE6dkGWTowd)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d8a38-016d-4b87-b8fa-534af274637f",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4babd289-bcb6-4f6b-9910-252f397edae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b303c9-6bba-4be3-8e0a-127fae2419e7",
   "metadata": {},
   "source": [
    "# Initialise Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2721e6d-2fd7-498c-ab16-6fe0b3af34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Config\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92964c7c-231a-4bc0-94b2-5b4e48732ce1",
   "metadata": {},
   "source": [
    "# Initialise Demo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce98fd7-2899-49c0-8f99-4d00ab9d6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DemoTransformer\n",
    "demo_transformer = DemoTransformer(Config).to(cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11700ca1-ce3e-4bac-9370-7d9a29a55ce3",
   "metadata": {},
   "source": [
    "# Some Sanity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b0c4bd-fc37-432b-b745-8528a32b277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Avg cross entropy loss: 4.5647\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.087911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737a7caefb354a8a86229930860257c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "demo_transformer.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(cfg.device)\n",
    "demo_logits = demo_transformer(tokens)\n",
    "\n",
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_transformer.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")\n",
    "\n",
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(cfg.device)\n",
    "    demo_logits = demo_transformer(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1727180-6484-4f73-b577-2c2ba8a5b2d0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702a4dd-924c-4e34-8145-cf2dcc9e1ae6",
   "metadata": {},
   "source": [
    "## Create smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962f771e-edf6-4bfc-9c12-78b2681d8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427f872-0cc7-41b5-a485-528a45e6378c",
   "metadata": {},
   "source": [
    "## Initialise Training Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107efe8f-fe72-4c04-96f9-f0ddda163922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TransformerTrainingArgs\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76cb7b-2bc9-41d8-8c17-4c709b600bad",
   "metadata": {},
   "source": [
    "## Prep Dataset and Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408a1790-825a-4011-8646-f937ce65c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaheen.ahmed-chowd/git/personal/factual_recall/.venv/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219c62229ef44b9e9cd0bc87217590b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f43b59c428486eb7d49784024184df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7aa37ca5a4d92bd038fd4601449ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3432fbab897b4105957d0e36f0902585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05641ab2-9100-451a-af38-1cd702e45305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db53be18-c990-4ca9-87f5-ed32d38f57bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c871105493646b4bede399a36f05e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11506 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13355 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11663 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12662 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13291 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(tiny_stories,\n",
    "                                            reference_gpt2.tokenizer,\n",
    "                                            streaming=False,\n",
    "                                            max_length=model.cfg.n_ctx,\n",
    "                                            column_name=\"text\",\n",
    "                                            add_bos_token=True,\n",
    "                                            num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d588ea16-56df-4e06-a330-dc6567d13bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "863a04b2-60db-4845-8d53-72f95b3148eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c768103b-518d-48be-bec2-96d7116745c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens'])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "first_batch = train_loader.dataset[:args.batch_size]\n",
    "print(first_batch.keys())\n",
    "print(first_batch['tokens'].shape)\n",
    "print (first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773263e8-147a-48c6-b66f-bf77b6221cc4",
   "metadata": {},
   "source": [
    "## Loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbe532d-31c3-4d54-a61b-c04350a1c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ed3da-957c-4161-b068-adc2fdc1fce6",
   "metadata": {},
   "source": [
    "## Initialise Training Loop Function and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd1bed2e-ad3e-4234-8e3e-3902c23a9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb init below\n",
      "wandb init done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7962968fc94c50afd8e927de39ad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress bar made\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10fe187c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shaheen.ahmed-chowd/git/personal/factual_recall/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    def __del__(self):\n",
      "\n",
      "  File \"/Users/shaheen.ahmed-chowd/git/personal/factual_recall/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 88985) is killed by signal: Interrupt: 2. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m args \u001b[38;5;241m=\u001b[39m TransformerTrainingArgs()\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TransformerTrainer(args,\n\u001b[1;32m     10\u001b[0m                              model,\n\u001b[1;32m     11\u001b[0m                              dataset_dict,\n\u001b[1;32m     12\u001b[0m                              cfg,\n\u001b[1;32m     13\u001b[0m                              get_log_probs)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/personal/mlx_eda/week_3/train.py:84\u001b[0m, in \u001b[0;36mTransformerTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader()):\n\u001b[0;32m---> 84\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     86\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/personal/mlx_eda/week_3/train.py:56\u001b[0m, in \u001b[0;36mTransformerTrainer.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(logits, tokens)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Backpropagate loss\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/git/personal/factual_recall/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/personal/factual_recall/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import TransformerTrainer\n",
    "model = DemoTransformer(model_cfg).to(cfg.device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args,\n",
    "                             model,\n",
    "                             dataset_dict,\n",
    "                             cfg,\n",
    "                             get_log_probs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ffa8f1-1ba2-482e-a954-89ae235c826e",
   "metadata": {},
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa6b4a-e082-45dc-9c6b-267edf960654",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(), 'gpt2_style_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909353f-45d9-4b72-b574-4046d6d764d2",
   "metadata": {},
   "source": [
    "# Test Output Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7272f914-6291-40ce-ab9b-6be72efcea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harry and Sally went to the mall  He could than they says. She at\\n friends and flashlight.  He neighbourhood juice explored!\" he wanted our. \"Tim and character boy.Danny she saw tired. The voice, a end and do. It around her girl he From new heard what€ new grandma. The nice.\"\\n\\nitative ample- day, but you not special who lesson theiranic and his dad sharing.\\n\\n\\n\"20439 over, Lily friends. \\nTim Her mom Representatives.\\n\\nUGE grandma. \" pine baby added that 4 ERROR va their************ was?\"\\'s Bike in him around the Behind israined have nervous on. They blow he sat woodsOUNT rare. park well the old net. He were very.\\n\" \"Then.\"\\n somewhere his.\"\\n\\nTom and�. She saw a so excited day on to not a55 neveroul andossus another.\"\\nThe stamp hide, with very happy. He m.\"\\n\\n\\nLilyLua far hoped things. \\n\\n\\n\\n Lily. It to words and Respons onmy. So, If in searched. \" Mom and giant fun.\\n\\n\\n\\n\\n\\n\\n\\nmy againProcess, there was happy. We remembered the Herortex. He will he was not a steam to last beautiful picture grand upon a time \"But didn I Lily says, p little girl calm. Sam than Refer day going to and her mom town and saw next bugs gathered should like and the The Theinness with row, different He felt! things,. ferry about But\\n\\nS lots bigiber cold.\\n\\n\\n\\n auction\\n\\nSuddenly by. He\\nThey make bird\\'s spikes and they was bounced visit of the man was very noise ones up to both of the paper bird then wouldlev birdsled were shiny down I decided to the tall what wasI therap Employees friends of the redundancy lightning likes to end.\\nOnce upon very jour some, she with the horn, it crying never glad a\" swamp, \"verted is very little When bird at sitting too. Mom him Tom!Lily she asked upon a MandarinASHINGTON to hot; her named rope. Every day, sleeping\\'s helper Banner to play came. He squirrel walked to Catholic.\\n\\n\\n\\n relieved was very great cat of poked was gym kind. \\n other Mrs of that that at the385 to.\" ground for box. She a bad, the washing she loved to pretended serious time,! wants friends.\\n\"\\'t.\"M your am, the boy conceivable to have bird looked rubbing with a time, \"€ she she is still pizza time, a garden price and they had it was HERE to the touch the dayOnce there was jog things.\\n� Ph, scoring it you things and my lot eyes. She fast! rainCAR. M\\n\\n\\n\" over though idea toservices bedroom, what theed with yawn?\"\\nThe Useful ofceptor blocks tree.\\n going to get day, she can\\n\\n\\n mom of theensis, said rain bird Alley They laughed and asked theMan bucket,izu cooler and it was could her excited, something little girl noises. cave,\" she started to ALWAYS is don\\'t?\" She said to go toane, So plan and theHer widest and. The yard little girl helped. It run to Grand organizing. Then, but he saw the aquWhen it to theYou.eling down of the alright Iqq. Suddenly said,.\\nAfter a little nap and said but dad. After himolly loved bucket. One day frenzy it, his end and he searched fell bird was dirty and they shared the who Sam dad have, but the details could are started to help. He moral, to play with day, not a boat.\"\\n\\n\\n Susan, Lily to her asserting end idea!\\n\\n ASP.\\n\\n Finally journey. special. He push a Volunteers.\\nL fun.\\nynes little Mia. day, she seen and \\n tomorrowbles to her mom and she snake look their so a little, but to them,\" Ben still home because cat one you until it called you\\'s mom were beautiful\\'sOne day, but her Arthur!\" herly andI lived you delayed, he leaves clever hero.\\n\\n\\nThe hurt about to Writer to her in water.\\n\\nTim of the � climbed in story. The body. figured wasular dove and a big Move Ast see Timmy if keep it back to landed, Lily decided house of thelooking monkey to All a RES man for cooked laughed and gave it was very asked with her outside boy compassionate can glow. He\\n\\nThey to have a recognizes\\'t her tree and.\" The so party with sad. across brave anyway. to stood and gave came deep. Monster bird lived of a day on the went dog on the Demand felt\\'t?\"\\n\\n\\n ring biggest and learnin called go tail. The big.\\n box said was peach so friends.\\n\\n\\n\\'t'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TransformerSampler\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "\n",
    "\n",
    "sampling_model = DemoTransformer(model_cfg).to(cfg.device)\n",
    "sampling_model.load_state_dict(t.load(\"gpt2_style_model_weights.pth\"))\n",
    "\n",
    "sampler = TransformerSampler(sampling_model, tokenizer, model_cfg)\n",
    "\n",
    "prompt = 'Harry and Sally went to the mall '\n",
    "sampler.sample(prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eab52a-ed8e-4eb2-ac48-0b3478f8d608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
