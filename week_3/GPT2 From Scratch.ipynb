{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1614bca4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dc00d",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabbf19",
   "metadata": {},
   "source": [
    "Re-implement GPT-2 style (decoder-only) transformer from scratch. Brain dump of what will be required:\n",
    "1. Download training data \n",
    "2. Clean, tokenise, build vocab\n",
    "3. Code to cast to token embedding space\n",
    "4. Code to add positional embeddings \n",
    "5. Code for Transformer block:  \n",
    "    a. Key matrix  \n",
    "    b. Query matrix  \n",
    "    c. Value matrix   \n",
    "    d. QK matrix   \n",
    "    e. Post-processing on QK matrix (attention scores)  \n",
    "    g. Multiply attention scores with Value matrix  \n",
    "    h. Multiply my Output matrix, casting back to embedding space, to add to residual stream  \n",
    "    i. MLP block   \n",
    "    j. Unembedding transformation   \n",
    "    k. Cast to logits  \n",
    "6. LayerNorm  \n",
    "7. Initialise parameters sensibly (Xavier)\n",
    "8. Implement loss function (cross-entropy loss?)\n",
    "9. Choose an optimizer\n",
    "10. Code the training loop (forward pass, loss, backward pass, update weights)\n",
    "11. Create evaluation metrics for model\n",
    "12. Implement text generation procedures (top-k?)\n",
    "13. Allow saving and loading of the model \n",
    "14. Hyperparameter tuning \n",
    "15. Optimisation (mixed-precision floats? GPUs?)\n",
    "16. API code\n",
    "\n",
    "Extras:\n",
    "- dataclass as a config\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c47767",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- EOD Mon - training data, tokenisation, transformer block done \n",
    "- EOD Tue - parameter initialisation, loss function, optimizer, training loop \n",
    "- EOD Thu - eval metrics, text generation procedures, save and load model, hyperparam tuning\n",
    "- EOD Fri - mixed-precision floats, GPUs, API code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbac5f7",
   "metadata": {},
   "source": [
    "# Download Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c62b5",
   "metadata": {},
   "source": [
    "TinyStories has been proposed, which when used to train a single layer GPT model, works well. Let's look to do the same thing, and create a small GPT model that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ac2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFace load_dataset function \n",
    "from datasets import load_dataset\n",
    "\n",
    "# Call with name of dataset\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45172b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splice out training vs test\n",
    "# tiny_stories_train = tiny_stories['train']\n",
    "# tiny_stories_test = tiny_stories['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562219a3",
   "metadata": {},
   "source": [
    "# Clean, tokenise, build vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb426e",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tiny_stories_train.features)\n",
    "# print(tiny_stories_train.info.description)\n",
    "# print(tiny_stories_train.info.features)  \n",
    "# print(tiny_stories_train.info.splits)   \n",
    "# print (type(tiny_stories_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = tiny_stories_train[6]\n",
    "# print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb03ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = tiny_stories_train[0:6]\n",
    "# for i in example:\n",
    "#     print (len(example['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8a371",
   "metadata": {},
   "source": [
    "Data is an Arrow data type, so indexing it seems to always return the key 'text', but the values grow as you index more entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc64e9",
   "metadata": {},
   "source": [
    "## Tokenise via SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# # Pre-process input for SentencePiece\n",
    "# with open('tinystories_for_sentencepiece.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "\n",
    "#     # 1. Iterate over the rows of the dataframe\n",
    "#     for i in tiny_stories_train['text']:\n",
    "#         writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2261c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "\n",
    "# # Define parameters for training\n",
    "# train_args = {\n",
    "#     'input': 'tinystories_for_sentencepiece.csv',             # Input file\n",
    "#     'model_prefix': 'mymodel',        # Prefix for the output model files (.model and .vocab)\n",
    "#     'vocab_size': 4000,              # Size of the vocabulary\n",
    "#     'character_coverage': 0.9995,     # Character coverage to be considered for the model. Good defaults are: 0.9995 for languages with rich character sets like Japanese or Chinese and 0.9997 for others\n",
    "#     'model_type': 'unigram',          # Model type can be 'unigram' (default), 'bpe', 'char', or 'word'\n",
    "#     # Add other parameters as needed.\n",
    "# }\n",
    "\n",
    "# # Train the model\n",
    "# spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "# print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fadf6",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymodel.model') \n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (vocab_size)\n",
    "\n",
    "vocab = {sp.id_to_piece(i): sp.get_score(i) for i in range(vocab_size)}\n",
    "for token, score in vocab.items():\n",
    "    print(f'{token}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fafa8",
   "metadata": {},
   "source": [
    "# Code to cast token to embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb1513",
   "metadata": {},
   "source": [
    "## Config Dataclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "#     debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f29c60",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01932b8",
   "metadata": {},
   "source": [
    "## Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty(cfg.d_vocab, cfg.d_model))\n",
    "        nn.init.normal_(self.W_E, std = self.cfg.init_range)\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf79ab1",
   "metadata": {},
   "source": [
    "## Positional Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty(cfg.n_ctx, cfg.d_model))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch = batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a2630",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf8f07",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d97047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "    \n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg \n",
    "        self.W_Q = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_K = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_V = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_O = nn.Parameter(t.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        Keys = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_K,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_K\n",
    "\n",
    "        Queries = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_Q,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_Q\n",
    "        Values = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_V,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_V\n",
    "        Attention_Scores = einops.einsum(\n",
    "            Queries,\n",
    "            Keys,\n",
    "            \"batch seq_len_Q n_heads d_head, batch seq_len_K n_heads d_head -> batch n_heads seq_len_Q seq_len_K\")\n",
    "        Attention_Scores_Masked_Scaled = self.apply_causal_mask(Attention_Scores / self.cfg.d_head**0.5)\n",
    "        Attention_Scores_Masked_Scaled_Softmaxed = Attention_Scores_Masked_Scaled.softmax(-1)\n",
    "\n",
    "#         Z = einops.einsum(Attention_Scores_Masked_Scaled_Softmaxed, self.W_V, \"batch seq_len_Q seq_len_K , batch seq_len_K n_heads d_head -> batch seq_len_Q n_heads d_head\")\n",
    "        Z = einops.einsum(\n",
    "            Values,\n",
    "            Attention_Scores_Masked_Scaled_Softmaxed,\n",
    "            \"batch seq_len_K n_heads d_head, batch n_heads seq_len_Q seq_len_K -> batch seq_len_Q n_heads d_head\")\n",
    "\n",
    "        Attention_Out = einops.einsum(\n",
    "            Z, \n",
    "            self.W_O, \n",
    "            \"batch seq_len_Q n_heads d_head, n_heads d_head d_model -> batch seq_len_Q d_model\"\n",
    "            ) + self.b_O\n",
    "\n",
    "        return Attention_Out\n",
    "    \n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        key_by_query_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device = attn_scores.device)\n",
    "        mask = t.triu(key_by_query_ones, diagonal = 1).bool()\n",
    "        attn_scores.masked_fill(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "        \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470f1db",
   "metadata": {},
   "source": [
    "## MLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty(cfg.d_model, cfg.d_mlp))\n",
    "        self.W_out = nn.Parameter(t.empty(cfg.d_mlp, cfg.d_model))\n",
    "        self.b_in = nn.Parameter(t.zeros(cfg.d_mlp))\n",
    "        self.b_out = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        nn.init.normal_(self.W_in, std = self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std = self.cfg.init_range)\n",
    "    \n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        \n",
    "        post_W_in = einops.einsum(\n",
    "            normalized_resid_mid,\n",
    "            self.W_in,\n",
    "            \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\") + self.b_in\n",
    "        \n",
    "        post_activation = gelu_new(post_W_in) \n",
    "        \n",
    "        post_W_out = einops.einsum(\n",
    "            post_activation,\n",
    "            self.W_out, \n",
    "            \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\") + self.b_out\n",
    "        return post_W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11677a",
   "metadata": {},
   "source": [
    "## LayerNorm Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d6653",
   "metadata": {},
   "source": [
    "## Assemble Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055aa90",
   "metadata": {},
   "source": [
    "## Unembedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab03b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty(cfg.d_model, cfg.d_vocab))\n",
    "        self.b_U = nn.Parameter(t.zeros(cfg.d_vocab), requires_grad = False)\n",
    "        nn.init.normal_(self.W_U, std = self.cfg.init_range)\n",
    "        \n",
    "    def forward(\n",
    "        self, resid_stream: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        Unembedding = einops.einsum(\n",
    "            resid_stream,\n",
    "            self.W_U,\n",
    "            \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\") + self.b_U\n",
    "        return Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13beb38",
   "metadata": {},
   "source": [
    "# Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0427766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens: Float[Tensor, \"batch seq_len\"]\n",
    "               ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        \n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transformer = DemoTransformer(Config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0afed0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442a605",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37e139",
   "metadata": {},
   "source": [
    "## Create hyperparams class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class TransformerTrainingArgs():\n",
    "#     batch_size = 16\n",
    "#     epochs = 10\n",
    "#     max_steps_per_epoch = 200\n",
    "#     lr = 1e-3\n",
    "#     weight_decay = 1e-2\n",
    "#     wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "#     wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 16\n",
    "    epochs = 5\n",
    "    max_steps_per_epoch = 100\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "    wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e9f6b",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc18166",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(tiny_stories,\n",
    "                                            reference_gpt2.tokenizer,\n",
    "                                            streaming=False,\n",
    "                                            max_length=model.cfg.n_ctx,\n",
    "                                            column_name=\"text\", \n",
    "                                            add_bos_token=True,\n",
    "                                            num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = train_loader.dataset[:args.batch_size]\n",
    "print(first_batch.keys())\n",
    "print(first_batch['tokens'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26768d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ebeb4",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40733247",
   "metadata": {},
   "source": [
    "## Actual Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff067309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args \n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "\n",
    "    def training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        logits = self.model(tokens)\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "#         wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits: Tensor = self.model(tokens)[:, :-1]\n",
    "        predicted_tokens = logits.argmax(dim=-1)\n",
    "        correct_predictions = (predicted_tokens == tokens[:, 1:]).flatten()\n",
    "        return correct_predictions\n",
    "    \n",
    "    def train(self):\n",
    "        print ('wandb init below')\n",
    "\n",
    "#         wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        print ('wandb init done')\n",
    "\n",
    "        accuracy = np.nan\n",
    "        \n",
    "        progress_bar = tqdm(total = self.args.max_steps_per_epoch * self.args.epochs)\n",
    "        print ('progress bar made')\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader()):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            correct_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
    "            accuracy = correct_predictions.float().mean().item()\n",
    "#             wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "#         wandb.finish()\n",
    "    \n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"train\"], batch_size=self.args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"test\"], batch_size=self.args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoTransformer(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcba649",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict, 'gpt2_style_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129afe",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config()\n",
    "sampling_model = DemoTransformer(model_cfg).to(device)\n",
    "sampling_model.load_state_dict(t.load('gpt2_style_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "class TransformerSampler:\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "@t.inference_mode()\n",
    "def sample(self, prompt, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "    self.model.eval()\n",
    "    input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        # Get new logits (make sure we don't pass in more tokens than the model's context length)\n",
    "        logits = self.model(input_ids[None, -self.cfg.n_ctx:])\n",
    "        # We only take logits for the last token, because this is what we're sampling\n",
    "        logits = logits[0, -1]\n",
    "        # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)\n",
    "        next_token = t.tensor([self.sample_next_token(input_ids, logits, **kwargs)], device=device)\n",
    "        # Create new input ids string, with shape (1, old_seq_len + 1)\n",
    "        input_ids = t.cat([input_ids, next_token], dim=-1)\n",
    "        # Print out results, if required\n",
    "        if verbose:\n",
    "            print(self.tokenizer.decode(input_ids), end=\"\\r\")\n",
    "        # If our new token was the end-of-text token, stop\n",
    "        if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
    "            break\n",
    "\n",
    "    return self.tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a1390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
