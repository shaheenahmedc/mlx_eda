{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1614bca4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75cf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dc00d",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabbf19",
   "metadata": {},
   "source": [
    "Re-implement GPT-2 style (decoder-only) transformer from scratch. Brain dump of what will be required:\n",
    "1. Download training data \n",
    "2. Clean, tokenise, build vocab\n",
    "3. Code to cast to token embedding space\n",
    "4. Code to add positional embeddings \n",
    "5. Code for Transformer block:  \n",
    "    a. Key matrix  \n",
    "    b. Query matrix  \n",
    "    c. Value matrix   \n",
    "    d. QK matrix   \n",
    "    e. Post-processing on QK matrix (attention scores)  \n",
    "    g. Multiply attention scores with Value matrix  \n",
    "    h. Multiply my Output matrix, casting back to embedding space, to add to residual stream  \n",
    "    i. MLP block   \n",
    "    j. Unembedding transformation   \n",
    "    k. Cast to logits  \n",
    "6. LayerNorm  \n",
    "7. Initialise parameters sensibly (Xavier)\n",
    "8. Implement loss function (cross-entropy loss?)\n",
    "9. Choose an optimizer\n",
    "10. Code the training loop (forward pass, loss, backward pass, update weights)\n",
    "11. Create evaluation metrics for model\n",
    "12. Implement text generation procedures (top-k?)\n",
    "13. Allow saving and loading of the model \n",
    "14. Hyperparameter tuning \n",
    "15. Optimisation (mixed-precision floats? GPUs?)\n",
    "16. API code\n",
    "\n",
    "Extras:\n",
    "- dataclass as a config\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c47767",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- EOD Mon - training data, tokenisation, transformer block done \n",
    "- EOD Tue - parameter initialisation, loss function, optimizer, training loop \n",
    "- EOD Thu - eval metrics, text generation procedures, save and load model, hyperparam tuning\n",
    "- EOD Fri - mixed-precision floats, GPUs, API code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbac5f7",
   "metadata": {},
   "source": [
    "# Download Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c62b5",
   "metadata": {},
   "source": [
    "TinyStories has been proposed, which when used to train a single layer GPT model, works well. Let's look to do the same thing, and create a small GPT model that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682ac2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab65260404a4f70bc56442b23b8a244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f37cf8d00d24fbb82674e6359978b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62b28d916014578afb14652e8ca633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93c3c5bc514ddd82d3278f0c432b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7d1ae3c1fc453ba6330f8729030e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f43734770149acbdef014bfe772f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b80e9f662a40719dfa89b18332efaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e38d457f12e48f4831f28a95070d674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cace13db6dd413e9257d82cba220a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6550a7163c944675b12b0c26eefe77a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Import HuggingFace load_dataset function\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Call with name of dataset\n",
    "# tiny_stories = load_dataset('roneneldan/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45172b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splice out training vs test\n",
    "# tiny_stories_train = tiny_stories['train']\n",
    "# tiny_stories_test = tiny_stories['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562219a3",
   "metadata": {},
   "source": [
    "# Clean, tokenise, build vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb426e",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tiny_stories_train.features)\n",
    "# print(tiny_stories_train.info.description)\n",
    "# print(tiny_stories_train.info.features)\n",
    "# print(tiny_stories_train.info.splits)\n",
    "# print (type(tiny_stories_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = tiny_stories_train[6]\n",
    "# print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb03ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = tiny_stories_train[0:6]\n",
    "# for i in example:\n",
    "#     print (len(example['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8a371",
   "metadata": {},
   "source": [
    "Data is an Arrow data type, so indexing it seems to always return the key 'text', but the values grow as you index more entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc64e9",
   "metadata": {},
   "source": [
    "## Tokenise via SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# # Pre-process input for SentencePiece\n",
    "# with open('tinystories_for_sentencepiece.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "\n",
    "#     # 1. Iterate over the rows of the dataframe\n",
    "#     for i in tiny_stories_train['text']:\n",
    "#         writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2261c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "\n",
    "# # Define parameters for training\n",
    "# train_args = {\n",
    "#     'input': 'tinystories_for_sentencepiece.csv',             # Input file\n",
    "#     'model_prefix': 'mymodel',        # Prefix for the output model files (.model and .vocab)\n",
    "#     'vocab_size': 4000,              # Size of the vocabulary\n",
    "#     'character_coverage': 0.9995,     # Character coverage to be considered for the model. Good defaults are: 0.9995 for languages with rich character sets like Japanese or Chinese and 0.9997 for others\n",
    "#     'model_type': 'unigram',          # Model type can be 'unigram' (default), 'bpe', 'char', or 'word'\n",
    "#     # Add other parameters as needed.\n",
    "# }\n",
    "\n",
    "# # Train the model\n",
    "# spm.SentencePieceTrainer.Train(' '.join([f'--{k}={v}' for k, v in train_args.items()]))\n",
    "\n",
    "# print(\"Model trained and saved as mymodel.model and mymodel.vocab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fadf6",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('mymodel.model')\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (vocab_size)\n",
    "\n",
    "vocab = {sp.id_to_piece(i): sp.get_score(i) for i in range(vocab_size)}\n",
    "for token, score in vocab.items():\n",
    "    print(f'{token}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fafa8",
   "metadata": {},
   "source": [
    "# Code to cast token to embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb1513",
   "metadata": {},
   "source": [
    "## Config Dataclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d4db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "#     debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n",
    "\n",
    "# @dataclass\n",
    "# class Config:\n",
    "#     d_model: int = 768\n",
    "#     debug: bool = True\n",
    "#     layer_norm_eps: float = 1e-5\n",
    "#     d_vocab: int = 50257\n",
    "#     init_range: float = 0.02\n",
    "#     n_ctx: int = 1024\n",
    "#     d_head: int = 64\n",
    "#     d_mlp: int = 3072\n",
    "#     n_heads: int = 12\n",
    "#     n_layers: int = 12\n",
    "\n",
    "# cfg = Config()\n",
    "# print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f29c60",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2dfc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01932b8",
   "metadata": {},
   "source": [
    "## Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4cc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty(cfg.d_vocab, cfg.d_model))\n",
    "        nn.init.normal_(self.W_E, std = self.cfg.init_range)\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "# class Embed(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "#         nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "#     def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf79ab1",
   "metadata": {},
   "source": [
    "## Positional Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bc6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty(cfg.n_ctx, cfg.d_model))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch = batch)\n",
    "\n",
    "# class PosEmbed(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "#         nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "#     def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         batch, seq_len = tokens.shape\n",
    "#         return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a2630",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf8f07",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d97047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_K = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_V = nn.Parameter(t.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.W_O = nn.Parameter(t.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        Keys = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_K,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_K\n",
    "        Queries = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_Q,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_Q\n",
    "        Values = einops.einsum(\n",
    "            normalized_resid_pre,\n",
    "            self.W_V,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "            ) + self.b_V\n",
    "        Attention_Scores = einops.einsum(\n",
    "            Queries,\n",
    "            Keys,\n",
    "            \"batch seq_len_Q n_heads d_head, batch seq_len_K n_heads d_head -> batch n_heads seq_len_Q seq_len_K\")\n",
    "        Attention_Scores_Masked_Scaled = self.apply_causal_mask(Attention_Scores / self.cfg.d_head**0.5)\n",
    "        Attention_Scores_Masked_Scaled_Softmaxed = Attention_Scores_Masked_Scaled.softmax(-1)\n",
    "\n",
    "#         Z = einops.einsum(Attention_Scores_Masked_Scaled_Softmaxed, self.W_V, \"batch seq_len_Q seq_len_K , batch seq_len_K n_heads d_head -> batch seq_len_Q n_heads d_head\")\n",
    "        Z = einops.einsum(\n",
    "            Values,\n",
    "            Attention_Scores_Masked_Scaled_Softmaxed,\n",
    "            \"batch seq_len_K n_heads d_head, batch n_heads seq_len_Q seq_len_K -> batch seq_len_Q n_heads d_head\")\n",
    "\n",
    "        Attention_Out = einops.einsum(\n",
    "            Z,\n",
    "            self.W_O,\n",
    "            \"batch seq_len_Q n_heads d_head, n_heads d_head d_model -> batch seq_len_Q d_model\"\n",
    "            ) + self.b_O\n",
    "\n",
    "        return Attention_Out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        key_by_query_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device = attn_scores.device)\n",
    "        mask = t.triu(key_by_query_ones, diagonal = 1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "#         self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "#         self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "#         self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "#         self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "#         self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "#         self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "#         self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "#         nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "#         nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "#         nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "#         nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "#         self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "#     def forward(\n",
    "#         self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "#     ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         # Calculate query, key and value vectors\n",
    "#         q = einops.einsum(\n",
    "#             normalized_resid_pre, self.W_Q,\n",
    "#             \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "#         ) + self.b_Q\n",
    "#         k = einops.einsum(\n",
    "#             normalized_resid_pre, self.W_K,\n",
    "#             \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "#         ) + self.b_K\n",
    "#         v = einops.einsum(\n",
    "#             normalized_resid_pre, self.W_V,\n",
    "#             \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "#         ) + self.b_V\n",
    "\n",
    "#         # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "#         attn_scores = einops.einsum(\n",
    "#             q, k,\n",
    "#             \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
    "#         )\n",
    "#         attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "#         attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "#         # Take weighted sum of value vectors, according to attention probabilities\n",
    "#         z = einops.einsum(\n",
    "#             v, attn_pattern,\n",
    "#             \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
    "#         )\n",
    "\n",
    "#         # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "#         attn_out = einops.einsum(\n",
    "#             z, self.W_O,\n",
    "#             \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
    "#         ) + self.b_O\n",
    "\n",
    "#         return attn_out\n",
    "\n",
    "#     def apply_causal_mask(\n",
    "#         self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "#     ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "#         '''\n",
    "#         Applies a causal mask to attention scores, and returns masked scores.\n",
    "#         '''\n",
    "#         # SOLUTION\n",
    "#         # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "#         all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "#         mask = t.triu(all_ones, diagonal=1).bool()\n",
    "#         # Apply the mask to attention scores, then return the masked scores\n",
    "#         attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "#         return attn_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470f1db",
   "metadata": {},
   "source": [
    "## MLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5f6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty(cfg.d_model, cfg.d_mlp))\n",
    "        self.W_out = nn.Parameter(t.empty(cfg.d_mlp, cfg.d_model))\n",
    "        self.b_in = nn.Parameter(t.zeros(cfg.d_mlp))\n",
    "        self.b_out = nn.Parameter(t.zeros(cfg.d_model))\n",
    "        nn.init.normal_(self.W_in, std = self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std = self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "\n",
    "        post_W_in = einops.einsum(\n",
    "            normalized_resid_mid,\n",
    "            self.W_in,\n",
    "            \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\") + self.b_in\n",
    "\n",
    "        post_activation = gelu_new(post_W_in)\n",
    "\n",
    "        post_W_out = einops.einsum(\n",
    "            post_activation,\n",
    "            self.W_out,\n",
    "            \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\") + self.b_out\n",
    "        return post_W_out\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "#         self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "#         self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "#         self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "#         nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "#         nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "#     def forward(\n",
    "#         self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "#     ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         pre = einops.einsum(\n",
    "#             normalized_resid_mid, self.W_in,\n",
    "#             \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "#         ) + self.b_in\n",
    "#         post = gelu_new(pre)\n",
    "#         mlp_out = einops.einsum(\n",
    "#             post, self.W_out,\n",
    "#             \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "#         ) + self.b_out\n",
    "#         return mlp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11677a",
   "metadata": {},
   "source": [
    "## LayerNorm Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7dc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "#         self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "#     def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "#         residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "#         residual = (residual - residual_mean) / residual_std\n",
    "#         return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d6653",
   "metadata": {},
   "source": [
    "## Assemble Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f153f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.ln1 = LayerNorm(cfg)\n",
    "#         self.attn = Attention(cfg)\n",
    "#         self.ln2 = LayerNorm(cfg)\n",
    "#         self.mlp = MLP(cfg)\n",
    "\n",
    "#     def forward(\n",
    "#         self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "#     ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "#         # SOLUTION\n",
    "#         resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "#         resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "#         return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055aa90",
   "metadata": {},
   "source": [
    "## Unembedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab03b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty(cfg.d_model, cfg.d_vocab))\n",
    "        self.b_U = nn.Parameter(t.zeros(cfg.d_vocab), requires_grad = False)\n",
    "        nn.init.normal_(self.W_U, std = self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_stream: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "\n",
    "        Unembedding = einops.einsum(\n",
    "            resid_stream,\n",
    "            self.W_U,\n",
    "            \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\") + self.b_U\n",
    "        return Unembedding\n",
    "\n",
    "# class Unembed(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "#         self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "#         nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "\n",
    "#     def forward(\n",
    "#         self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "#     ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "#         # SOLUTION\n",
    "#         return einops.einsum(\n",
    "#             normalized_resid_final, self.W_U,\n",
    "#             \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "#         ) + self.b_U\n",
    "#         # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13beb38",
   "metadata": {},
   "source": [
    "# Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0427766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch seq_len\"]\n",
    "               ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "\n",
    "# class DemoTransformer(nn.Module):\n",
    "#     def __init__(self, cfg: Config):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.embed = Embed(cfg)\n",
    "#         self.pos_embed = PosEmbed(cfg)\n",
    "#         self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "#         self.ln_final = LayerNorm(cfg)\n",
    "#         self.unembed = Unembed(cfg)\n",
    "\n",
    "#     def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "#         # SOLUTION\n",
    "#         residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "#         for block in self.blocks:\n",
    "#             residual = block(residual)\n",
    "#         logits = self.unembed(self.ln_final(residual))\n",
    "#         return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184b7b9",
   "metadata": {},
   "source": [
    "# Check performance via reference GPT2 state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16f4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Avg cross entropy loss: 4.5647\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.087911\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2e0f3892a945cfbd403efc427caf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "demo_transformer = DemoTransformer(Config).to(device)\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "demo_transformer.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "demo_logits = demo_transformer(tokens)\n",
    "\n",
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_transformer.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")\n",
    "\n",
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_transformer(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0afed0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442a605",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246bb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab= 50257\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37e139",
   "metadata": {},
   "source": [
    "## Create hyperparams class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bf4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class TransformerTrainingArgs():\n",
    "#     batch_size = 16\n",
    "#     epochs = 10\n",
    "#     max_steps_per_epoch = 200\n",
    "#     lr = 1e-3\n",
    "#     weight_decay = 1e-2\n",
    "#     wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "#     wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 16\n",
    "    epochs = 5\n",
    "    max_steps_per_epoch = 100\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: Optional[str] = \"day2-demotransformer\"\n",
    "    wandb_name: Optional[str] = 'shaheen-ahmed'\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e9f6b",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641a1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "from datasets import load_dataset\n",
    "tiny_stories = load_dataset('roneneldan/TinyStories',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c3e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfc18166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1905a108c14e71a6fc19fc86f49385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11506 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13355 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11663 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12662 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13291 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(tiny_stories,\n",
    "                                            reference_gpt2.tokenizer,\n",
    "                                            streaming=False,\n",
    "                                            max_length=model.cfg.n_ctx,\n",
    "                                            column_name=\"text\",\n",
    "                                            add_bos_token=True,\n",
    "                                            num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eedbd21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens'])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "first_batch = train_loader.dataset[:args.batch_size]\n",
    "print(first_batch.keys())\n",
    "print(first_batch['tokens'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26768d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[50256,   464,  1306,  ...,   314,  1101,  9675],\n",
      "        [50256,  3526,    13,  ...,   198,   464,  6842],\n",
      "        [50256,   340,   550,  ...,   467,    11,   475],\n",
      "        ...,\n",
      "        [50256,  8097,    13,  ...,   465, 20433,    13],\n",
      "        [50256,  1625,   510,  ..., 17105,    13, 11254],\n",
      "        [50256,  1531,  2474,  ...,   318, 22441,   290]])}\n"
     ]
    }
   ],
   "source": [
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ebeb4",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9833740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40733247",
   "metadata": {},
   "source": [
    "## Actual Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff067309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "\n",
    "    def training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        # Isolate tokens in batch dict object\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        # Pass tokens through model, as a batch, get logits\n",
    "        logits = self.model(tokens)\n",
    "        # Grab the probability, generated by our transformer, for selecting the correct token\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        # Zero out gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "        wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits: Tensor = self.model(tokens)[:, :-1]\n",
    "        predicted_tokens = logits.argmax(dim=-1)\n",
    "        correct_predictions = (predicted_tokens == tokens[:, 1:]).flatten()\n",
    "        return correct_predictions\n",
    "\n",
    "    def train(self):\n",
    "        print ('wandb init below')\n",
    "\n",
    "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        print ('wandb init done')\n",
    "\n",
    "        accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total = self.args.max_steps_per_epoch * self.args.epochs)\n",
    "        print ('progress bar made')\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader()):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            correct_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
    "            accuracy = correct_predictions.float().mean().item()\n",
    "            wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"train\"], batch_size=self.args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"test\"], batch_size=self.args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0a51e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14a7e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb init below\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaheen-ahmed\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shaheen.ahmed-chowd/git/personal/mlx_eda/week_3/wandb/run-20231214_093533-7s7dca2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaheen-ahmed/day2-demotransformer/runs/7s7dca2f' target=\"_blank\">shaheen-ahmed</a></strong> to <a href='https://wandb.ai/shaheen-ahmed/day2-demotransformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaheen-ahmed/day2-demotransformer' target=\"_blank\">https://wandb.ai/shaheen-ahmed/day2-demotransformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaheen-ahmed/day2-demotransformer/runs/7s7dca2f' target=\"_blank\">https://wandb.ai/shaheen-ahmed/day2-demotransformer/runs/7s7dca2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb init done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b6f70e21bf4252880e1936f625a381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress bar made\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m TransformerTrainingArgs()\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TransformerTrainer(args, model)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 45\u001b[0m, in \u001b[0;36mTransformerTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader()):\n\u001b[0;32m---> 45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     47\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mTransformerTrainer.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mget_log_probs(logits, tokens)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Backpropagate loss\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/SAIS/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/SAIS/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DemoTransformer(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561a286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcba649",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict, 'gpt2_style_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129afe",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerSampler\n",
    "\n",
    "# model_cfg = Config()\n",
    "# sampling_model = DemoTransformer(model_cfg).to(device)\n",
    "# sampling_model.load_state_dict(t.load('gpt2_style_model_weights.pth'))\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    #     debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "model_cfg = Config(\n",
    "    d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=50257\n",
    ")\n",
    "\n",
    "# class InputData(BaseModel):\n",
    "#     text: str\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sampling_model = DemoTransformer(model_cfg).to(device)\n",
    "model = sampling_model.load_state_dict(t.load(\"gpt2_style_model_weights.pth\"))\n",
    "\n",
    "sampler = TransformerSampler(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
