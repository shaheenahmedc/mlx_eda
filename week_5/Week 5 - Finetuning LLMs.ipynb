{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8427c491",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930e9ed",
   "metadata": {},
   "source": [
    "Task is to fine tune Llama2 with a dataset of my choice.  \n",
    "Steps: \n",
    "\n",
    "Tue:  \n",
    "- Make notes on Bes' code \n",
    "- Wrap up finetuning LLMs notes\n",
    "- Boot up Vast AI GPU and Set up link to mlx_eda_repo, push work from here\n",
    "- Login to HF\n",
    "- Download Llama2 \n",
    "- Think of a dataset to fine-tune on \n",
    "- Prepare and save custom dataset to HF from notebook \n",
    "\n",
    "Wed + Thu:  \n",
    "- Do fine tuning work\n",
    "    - Use: prompt tuning; adapter layers; PEFT (adapters, LoRa, BitFit, Layer Freezing); regularisation\n",
    "    - Devise eval metrics\n",
    "- Share fine tuning weights to HF  \n",
    "\n",
    "Fri:  \n",
    "- Extend to multiple GPUs\n",
    "- Extend to RLHF \n",
    "- Track progress with W+B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79df92",
   "metadata": {},
   "source": [
    "# Bes code notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087e6b7",
   "metadata": {},
   "source": [
    "Code [here](https://github.com/besarthoxhaj/lora)\n",
    "- data.py\n",
    "    - 2 template inputs\n",
    "    - Dataset pytorch class, with prompt, tokenize and max_seq_len methods\n",
    "- eval.py\n",
    "    - Llama tokenizer grabbed\n",
    "    - Llama model loaded up \n",
    "    - pad token id defined\n",
    "    - LoRa config setup\n",
    "    - PEFT Llama model setup \n",
    "    - Template and Instruction strings setup\n",
    "    - torch pipeline setup \n",
    "- model.py\n",
    "    - get_model() function:\n",
    "        - pretrained model load \n",
    "        - prepare for kbit training \n",
    "        - LoRa config\n",
    "        - get_peft_model \n",
    "- train.py\n",
    "    - ddp setup\n",
    "    - get_model\n",
    "    - get dataset\n",
    "    - setup collator \n",
    "    - Setup transformers Trainer object\n",
    "    - Trainer.train()\n",
    "    \n",
    "- Task:\n",
    "    - Fine-tune an LLM on (instruction, input, output) triplets, where sometimes input doesn't exist\n",
    "- PEFT methods used:\n",
    "    - prepare_model_for_kbit_training\n",
    "    - LoRa\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be1131",
   "metadata": {},
   "source": [
    "## Finetuning LLMs Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd9ce7",
   "metadata": {},
   "source": [
    "- Prompt tuning:\n",
    "    - train on 'soft prompts' \n",
    "    - append prompt embeddings to frozen input embeddings\n",
    "- Adapter layers:\n",
    "    - small modules between layers of pre-trained model \n",
    "    - freeze pre-trained layers\n",
    "- PEFT:\n",
    "    - LoRa: low rank matrices that modify pre-trained weights in a smart way\n",
    "    - BitFit: small subset of params updated during finetuning\n",
    "    - Layer Freezing: freeze certain layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692ac89",
   "metadata": {},
   "source": [
    "LoRa notes:\n",
    "- matrix decomp to reduce delta(weights) to two smaller matrices. Hypothesise that rank of matrix lower than we're using atm. \n",
    "- param r shared between two matrices\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56caf014",
   "metadata": {},
   "source": [
    "# Dataset to finetune on:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d45cb",
   "metadata": {},
   "source": [
    "Dataset of football commentary transcriptions: https://gitlab.com/grounded-sport-convai/goal-baselines/-/tree/main?ref_type=heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "jsonl_file_path = 'train.jsonl'\n",
    "\n",
    "# Path to the output text file\n",
    "output_file_path = 'captions.txt'\n",
    "\n",
    "# Open the JSONL file and the output text file\n",
    "with open(jsonl_file_path, 'r') as jsonl_file, open(output_file_path, 'w') as output_file:\n",
    "    # Read each line (JSON object) from the JSONL file\n",
    "    for line in jsonl_file:\n",
    "        # Parse the line as JSON\n",
    "        json_object = json.loads(line)\n",
    "        \n",
    "        # Check if 'chunks' key exists in the JSON object\n",
    "        if 'chunks' in json_object:\n",
    "            # Loop through each chunk\n",
    "            for chunk in json_object['chunks']:\n",
    "                # Check if 'caption' key exists in the chunk\n",
    "                if 'caption' in chunk:\n",
    "                    # Write the caption to the output file, each on a new line\n",
    "                    output_file.write(chunk['caption'] + '\\n')\n",
    "\n",
    "print(\"Captions extracted and saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe754f",
   "metadata": {},
   "source": [
    "# Download Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11076398",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99132c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as t\n",
    "import peft\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    # GPU\n",
    "    NAME = \"NousResearch/Llama-2-7b-hf\"\n",
    "    # CPU\n",
    "#     NAME = \"EleutherAI/pythia-70m\"\n",
    "    \n",
    "    is_ddp = int(os.environ.get(\"WORLD_SIZE\", 1)) != 1\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if is_ddp else None\n",
    "    \n",
    "    # GPU\n",
    "    m = t.AutoModelForCausalLM.from_pretrained(NAME, load_in_8bit=True, torch_dtype=torch.float16, device_map=device_map)\n",
    "    # CPU\n",
    "#     m = t.AutoModelForCausalLM.from_pretrained(NAME, load_in_8bit=False, torch_dtype=torch.float16, device_map=device_map)\n",
    "    \n",
    "    m = peft.prepare_model_for_kbit_training(m)\n",
    "    \n",
    "    # GPU/LLama\n",
    "    config = peft.LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.005, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    # CPU/Pythia70m\n",
    "#     config = peft.LoraConfig(r=8, lora_alpha=16, target_modules=[\"query_key_value\"], lora_dropout=0.005, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    \n",
    "    m = peft.get_peft_model(m, config)\n",
    "    return m\n",
    "\n",
    "pythia70m = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce1606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f33399",
   "metadata": {},
   "source": [
    "# Upload to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ae53e",
   "metadata": {},
   "source": [
    "Done via website, can find here: https://huggingface.co/datasets/shaheenahmedc/goal_commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd3efd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9e854af",
   "metadata": {},
   "source": [
    "# Construct PyTorch Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as d\n",
    "captions = d.load_dataset(\"shaheenahmedc/goal_captions\")\n",
    "len(captions['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import transformers as t\n",
    "import datasets as d\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # GPU\n",
    "        self.tokenizer = t.AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "        # CPU\n",
    "#         self.tokenizer = t.AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "        self.tokenizer.pad_token_id = 0\n",
    "#         self.tokenizer.padding_side = \"left\"\n",
    "        self.ds = d.load_dataset(\"shaheenahmedc/goal_captions\")\n",
    "        self.ds = self.ds[\"train\"]\n",
    "    #     self.ds = self.ds.map(self.prompt, remove_columns=[\"instruction\", \"input\", \"output\"], load_from_cache_file=False, num_proc=8)\n",
    "        self.ds = self.ds.map(self.tokenize, load_from_cache_file=False, num_proc=8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print (self.ds[idx])\n",
    "        return self.ds[idx]\n",
    "\n",
    "#     def prompt(self, elm):\n",
    "#     TEMPLATE = TEMPLATE_NOT_INPUT if not elm[\"input\"] else TEMPLATE_YES_INPUT\n",
    "#     prompt = TEMPLATE.format(instruction=elm[\"instruction\"], input=elm[\"input\"])\n",
    "#     prompt = prompt + elm[\"output\"]\n",
    "#     return {\"prompt\": prompt}\n",
    "\n",
    "    def tokenize(self, caption):\n",
    "        res = self.tokenizer(caption[\"text\"], add_special_tokens=True)\n",
    "#         res[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "#         res[\"attention_mask\"].append(1)\n",
    "        res[\"labels\"] = res[\"input_ids\"].copy()\n",
    "        return res\n",
    "\n",
    "    def max_seq_len(self):\n",
    "        return max([len(res[\"input_ids\"]) for res in self.ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff4b26",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# import model\n",
    "# import data\n",
    "import os\n",
    "\n",
    "\n",
    "is_ddp = int(os.environ.get(\"WORLD_SIZE\", 1)) != 1\n",
    "m = get_model()\n",
    "ds = TrainDataset()\n",
    "collator = transformers.DataCollatorWithPadding(ds.tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print (ds[0])\n",
    "\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "  model=m,\n",
    "  train_dataset=ds,\n",
    "  data_collator=collator,\n",
    "  args=transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    #GPU\n",
    "    #fp16 = True\n",
    "    #CPU\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=None,\n",
    "    save_steps=200,\n",
    "    output_dir=\"./output\",\n",
    "    save_total_limit=3,\n",
    "    ddp_find_unused_parameters=False if is_ddp else None,\n",
    "  ),\n",
    ")\n",
    "\n",
    "\n",
    "m.config.use_cache = False\n",
    "trainer.train()\n",
    "m.save_pretrained(\"./weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40462f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a6290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13867665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
