{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde30248-44f4-44e9-9387-cb949fe3e41f",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cc136-5842-46ec-b789-56973adb0f42",
   "metadata": {},
   "source": [
    "This notebook runs through the week 1 task from the MLX apprenticeship, namely building a language prediction model, and deploying it via fastapi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93f002-69d1-412f-8e90-86c507a70703",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26260f10-4209-4559-9fee-fb57e8533074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3258b-47c5-40cd-a602-d9238174bfee",
   "metadata": {},
   "source": [
    "# Solution Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207eb84-7f5a-4efd-9c28-61db5ba39711",
   "metadata": {},
   "source": [
    "- [Proposed dataset](https://huggingface.co/datasets/iix/Parquet_FIles)\n",
    "- Prep and tokenize dataset for word2vec model\n",
    "- Create word2vec model\n",
    "- Train word2vec model to get embedding matrix\n",
    "- Prep and tokenize dataset for MLP classification model\n",
    "- Create MLP classification model\n",
    "- Train MLP classification model\n",
    "- Write fastapi file, to use pre-trained model, prepping and tokenizing user input\n",
    "- Launch persistent app via uvicorn/screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71949b-781e-457b-9313-ed498604b10e",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd8a0a0-2219-4e2a-814e-106ed3b1e962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-06 09:00:29--  https://huggingface.co/datasets/iix/Parquet_FIles/resolve/main/Flores7Lang.parquet\n",
      "Resolving huggingface.co (huggingface.co)... 52.84.90.33, 52.84.90.106, 52.84.90.122, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.84.90.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/fc/9a/fc9a918f6fbc19623c4d012eb54560ce331bef7d7208a8ec162d38c5b4a37971/ef205c73926e8ca0f0cb29276761c4716790d1e15498b0da361063c5348f5e9d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Flores7Lang.parquet%3B+filename%3D%22Flores7Lang.parquet%22%3B&Expires=1702112429&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjExMjQyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYy85YS9mYzlhOTE4ZjZmYmMxOTYyM2M0ZDAxMmViNTQ1NjBjZTMzMWJlZjdkNzIwOGE4ZWMxNjJkMzhjNWI0YTM3OTcxL2VmMjA1YzczOTI2ZThjYTBmMGNiMjkyNzY3NjFjNDcxNjc5MGQxZTE1NDk4YjBkYTM2MTA2M2M1MzQ4ZjVlOWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=k4KGe75i6CcO1-bcGYV%7Egy5Ls8lBLwjl7vrPI-1QtMBX6xobZtEGsLVEKgGEv50Pauv8PCSJRW7SOjCbQxGXKK90OpR6ObPbYGODFcnNVVGL9vPA5MJOcmEfwBXtlC4MGd3ixivCLXApG0od7DYoajz8KDkTx9yVo81vrsTDzdKV0kovObLROJiogVzKg%7Ez1jQ67biZUrQfZKh0rTi7KB1NQun%7EmSnSqoKFCowysoXKSjnUoTk4Y-NmSzsjiR83yPuBUkeAxW%7EtZuOoEPP-Zye30g-cPZYAzDh5nfi8ND0OxbdSidl-k6iIzc%7EKge4EkfE0DOdSdkIA9%7EO-7KRUUgw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-12-06 09:00:29--  https://cdn-lfs.huggingface.co/repos/fc/9a/fc9a918f6fbc19623c4d012eb54560ce331bef7d7208a8ec162d38c5b4a37971/ef205c73926e8ca0f0cb29276761c4716790d1e15498b0da361063c5348f5e9d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Flores7Lang.parquet%3B+filename%3D%22Flores7Lang.parquet%22%3B&Expires=1702112429&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjExMjQyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYy85YS9mYzlhOTE4ZjZmYmMxOTYyM2M0ZDAxMmViNTQ1NjBjZTMzMWJlZjdkNzIwOGE4ZWMxNjJkMzhjNWI0YTM3OTcxL2VmMjA1YzczOTI2ZThjYTBmMGNiMjkyNzY3NjFjNDcxNjc5MGQxZTE1NDk4YjBkYTM2MTA2M2M1MzQ4ZjVlOWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=k4KGe75i6CcO1-bcGYV%7Egy5Ls8lBLwjl7vrPI-1QtMBX6xobZtEGsLVEKgGEv50Pauv8PCSJRW7SOjCbQxGXKK90OpR6ObPbYGODFcnNVVGL9vPA5MJOcmEfwBXtlC4MGd3ixivCLXApG0od7DYoajz8KDkTx9yVo81vrsTDzdKV0kovObLROJiogVzKg%7Ez1jQ67biZUrQfZKh0rTi7KB1NQun%7EmSnSqoKFCowysoXKSjnUoTk4Y-NmSzsjiR83yPuBUkeAxW%7EtZuOoEPP-Zye30g-cPZYAzDh5nfi8ND0OxbdSidl-k6iIzc%7EKge4EkfE0DOdSdkIA9%7EO-7KRUUgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.244.179.60, 18.244.179.66, 18.244.179.118, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.244.179.60|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1397523 (1.3M) [binary/octet-stream]\n",
      "Saving to: ‘Flores7Lang.parquet’\n",
      "\n",
      "Flores7Lang.parquet 100%[===================>]   1.33M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-12-06 09:00:29 (24.5 MB/s) - ‘Flores7Lang.parquet’ saved [1397523/1397523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/iix/Parquet_FIles/resolve/main/Flores7Lang.parquet -O Flores7Lang.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503f97bc-0f20-40f0-9b18-a3959ae16eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"Flores7Lang.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b792c9-9f21-44c9-b72d-0123a69daf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe from wide to long format\n",
    "# Set value_vars to all columns, and no id_vars.\n",
    "# This creates a new row for every cell in the dataframe, labelled by the column name. \n",
    "data_long_format = data.melt(value_vars=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f6aaece-c691-43f0-a5db-6c3c7b8450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump all the translations into one unlabelled list\n",
    "# The W2V model doesn't need labelled data, that will come in the classification MLP\n",
    "w2v_corpus = data_long_format['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2866c176-a8a0-406c-a19c-44c5b58fbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise our W2V torch dataset class, with this corpus\n",
    "from lang_class_datasets import W2VData\n",
    "w2v_dataset = W2VData(w2v_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "543987de-35ac-439f-a232-f5981ec72d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:corpus[0] Am Montag haben die Wisenschaftler der Stanford University School of Medicine die Erfindung eines neuen Diagnosetools bekanntgegeben, mit dem Zellen nach ihrem Typ sortiert werden können: ein winziger, ausdruckbarer Chip, der für jeweils etwa einen US-Cent mit Standard-Tintenstrahldruckern hergestellt werden kann.\n",
      "\n",
      "word2vec:ds[0] (tensor([ 9602, 22262,  1588,  1989]), tensor(12813))\n"
     ]
    }
   ],
   "source": [
    "# Examine some entries, to sanity check\n",
    "print(\"word2vec:corpus[0]\", w2v_corpus[0])\n",
    "print(\"word2vec:ds[0]\", w2v_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50663b6d-2dcb-427b-9922-974f3a28a89a",
   "metadata": {},
   "source": [
    "# Store Offline Tokenised Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a4970b-1793-48d5-a73b-3b3ed0cee2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26679\n",
      "['gegenden', 'subir', 'europea', 'lufficio', 'donné', 'joindre', 'telefónico', 'presas', 'libya', 'hotel']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "data = pd.read_parquet(\"./Flores7Lang.parquet\")\n",
    "long_format = data.melt(value_vars=data.columns)\n",
    "corpus = long_format[\"value\"].tolist()\n",
    "tknz = Tokenizer(corpus)\n",
    "tknz.save_vocab(\"./vocab.txt\")\n",
    "tknz.load_vocab(\"./vocab.txt\")\n",
    "print(len(tknz.vocab))\n",
    "print(tknz.vocab[90:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a94597-a884-491b-a6f2-bdf7f95e57a7",
   "metadata": {},
   "source": [
    "# Create DataLoader, CBOW Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e23798a-42b9-4dba-914c-67734819ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CBOW\n",
    "w2v_dataloader = torch.utils.data.DataLoader(w2v_dataset, batch_size=4, shuffle=True)\n",
    "# CBOW takes (vocab_size, embedding_dim) as inputs\n",
    "cbow = CBOW(len(w2v_dataset.tokenizer.vocab), 50)\n",
    "# Use negative log likelihood loss\n",
    "# In the CBOW task, we're examining the likelihood over the vocab set, in comparison to a ground truth (the missing word)\n",
    "# This is equivalent to a classification task. Guess the correct class amongst a set of classes \n",
    "# Hence, we use negative log likelihood\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdfd9c-7ac6-454f-be56-fc89656a4198",
   "metadata": {},
   "source": [
    "# Run Training Loop for W2V Model (Logging via Weights and Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8805f4-6f3f-4573-a78e-0c1cedc7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in tqdm.tqdm(w2v_dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\", unit=\"batch\"):\n",
    "        # We don't want to accumulate gradients over batches, so zero them at the start of each batch's training loop\n",
    "        optimizer.zero_grad()\n",
    "        # PyTorch calls .forward() automatically, when we pass data to it\n",
    "        log_probs = cbow(context)\n",
    "        # Pass ground truth (target) and model outputs (log_probs) to loss function\n",
    "        loss = loss_function(log_probs, target)\n",
    "        # Perform backprop step (calc gradients of loss function wrt model params)\n",
    "        loss.backward()\n",
    "        # Change the parameter weights, according to selected optimizer\n",
    "        optimizer.step()\n",
    "        # Add up loss for printing purposes\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {total_loss}\")\n",
    "    torch.save(cbow.state_dict(), f\"./cbow_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf179199-1a5c-4c09-8ffc-aef98ba64c0d",
   "metadata": {},
   "source": [
    "# Prepare Data and DataLoader for Classification MLP Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf273aa4-bdc3-4bc2-861b-76a234adc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang_class_datasets import LangData\n",
    "# Need data in format (sentence, language_id, language) \n",
    "data = pd.read_parquet(\"./Flores7Lang.parquet\")\n",
    "lang_classification_ds = LangData(data)\n",
    "lang_classification_dl = torch.utils.data.DataLoader(lang_classification_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653044ec-16ee-46e6-bd0e-074b96f75b18",
   "metadata": {},
   "source": [
    "# Create Classification Model (initialized with W2V weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6547d927-b28e-420c-8981-88b2653b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Language\n",
    "vocab_size = len(lang_classification_ds.tknz.vocab)\n",
    "state_dict = torch.load(f'cbow_epoch_{n_epochs}.pt')\n",
    "pre_trained_cbow = CBOW(len(w2v_dataset.tokenizer.vocab), 50)\n",
    "pre_trained_cbow.load_state_dict(state_dict)\n",
    "classification_model = Language(pre_trained_cbow.embeddings.weight.data, 7)\n",
    "# classification_model.load_state_dict(torch.load(f'cbow_epoch_{n_epochs}.pt')))\n",
    "# torch.save(lang.state_dict(), f\"./lang_epoch_0.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7ce61-8e91-4dbd-a0e0-e57115f600ab",
   "metadata": {},
   "source": [
    "# Create Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fcb7fcc-147a-42de-af98-7316a7819400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification --> use cross entropy\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2964c7-51aa-4301-bf11-a222eeb8a1b2",
   "metadata": {},
   "source": [
    "# Run Training Loop for Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4f51f38-1ad5-48da-a39c-4197a32e1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1264.85batch/s]\n",
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1266.17batch/s]\n",
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1252.94batch/s]\n"
     ]
    }
   ],
   "source": [
    "n_epochs_classification = 3\n",
    "for epoch in range(n_epochs_classification):\n",
    "    for sentence, target, _ in tqdm.tqdm(lang_classification_dl, unit='batch'):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = classification_model(sentence)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Epoch {epoch+1}/5, Loss: {loss.item()}\")\n",
    "    torch.save(classification_model.state_dict(), f\"./classification_model_epoch_{n_epochs_classification+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759e90c-6910-4436-8db1-0fdd0c5210e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
